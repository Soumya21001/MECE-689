{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /Users/dd/anaconda3/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/dd/anaconda3/lib/python3.11/site-packages (from gymnasium) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/dd/anaconda3/lib/python3.11/site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/dd/anaconda3/lib/python3.11/site-packages (from gymnasium) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/dd/anaconda3/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: False\n",
      "Tensor is on device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check if GPU is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f\"GPU Available: {gpu_available}\")\n",
    "\n",
    "# If GPU is available, print the GPU name\n",
    "if gpu_available:\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Check if tensors are being moved to GPU\n",
    "device = torch.device(\"cuda\" if gpu_available else \"cpu\")\n",
    "tensor = torch.rand(3, 3).to(device)\n",
    "print(f\"Tensor is on device: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Get the initial state\n",
    "state, info = env.reset()   \n",
    "num_bins=10                         # Number of bins to Discretize\n",
    "num_actions = env.action_space.n    # Number of actions (left, right)\n",
    "state_space_size = (num_bins,) * 4  # State space size after discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper Functions for Temporal Difference (TD) Learning with Linear Function Approximation\n",
    "\"\"\"\n",
    "\n",
    "def extract_features(state):\n",
    "    \"\"\"\n",
    "    Function to extract features from the state\n",
    "    \"\"\"\n",
    "    cart_pos, cart_vel, pole_angle, pole_vel = state\n",
    "    return np.array([cart_pos / 2.4,cart_vel / 2.0,pole_angle / 0.2095,pole_vel / 3.0])\n",
    "\n",
    "def choose_action_linear(weights, state, exploration_rate):\n",
    "    \"\"\"\n",
    "    Function to choose an action using epsilon-greedy policy\n",
    "    \"\"\"\n",
    "    if np.random.rand() < exploration_rate:\n",
    "        # Choose a random action\n",
    "        return np.random.randint(num_actions)\n",
    "    else: \n",
    "        # Choose the action with the highest Q-value\n",
    "        q_values = [np.dot(weights[a], extract_features(state)) for a in range(env.action_space.n)]\n",
    "        return np.argmax(q_values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 2.1: Temporal Difference (TD) Learning with Linear Function Approximation (SARSA)\n",
    "\"\"\"\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01  \n",
    "discount_factor = 0.90  \n",
    "num_features = 4            # Number of state features\n",
    "\n",
    "# Initialize weights for linear approximation\n",
    "weights_sarsa = np.zeros((env.action_space.n, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes, exploration_rate):\n",
    "    \"\"\"\n",
    "    Function to perform SARSA with linear function approximation\n",
    "    \"\"\"\n",
    "\n",
    "    # Loop over episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment and choose the initial action\n",
    "        state, _ = env.reset()\n",
    "        action = choose_action_linear(weights_sarsa, state, exploration_rate)\n",
    "        \n",
    "        # Initialize metrics\n",
    "        total_reward = 0  # Tracking total reward \n",
    "        total_td_error = 0  # Tracking total TD error \n",
    "\n",
    "        # Loop over steps within the episode\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Take a step in the environment and choose the next action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_action = choose_action_linear(weights_sarsa, next_state, exploration_rate)\n",
    "\n",
    "            # Compute the Q-values for the current state\n",
    "            features = extract_features(state)\n",
    "            q_value = np.dot(weights_sarsa[action], features)\n",
    "\n",
    "            # Compute the Q-value for the next state\n",
    "            next_features = extract_features(next_state)\n",
    "            next_q_value = np.dot(weights_sarsa[next_action], next_features)\n",
    "\n",
    "            # Compute the TD error\n",
    "            td_error = np.add(reward, np.multiply(discount_factor, next_q_value)) - q_value\n",
    "\n",
    "            # Update the weights using the TD error\n",
    "            weights_sarsa[action] += learning_rate * td_error * features\n",
    "\n",
    "            # Accumulate reward and TD error\n",
    "            total_reward += reward\n",
    "            total_td_error += abs(td_error)\n",
    "\n",
    "            # Move to the next state and action\n",
    "            state, action = next_state, next_action\n",
    "            done = terminated or truncated\n",
    "\n",
    "        # Print metrics every 100 episodes\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            print(f\"SARSA Episode {episode + 1}: \\tTotal Reward = {total_reward:.2f}, \"\n",
    "                  f\"\\tTotal TD Error = {total_td_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SARSA with Linear Function Approximation...\n",
      "SARSA Episode 1000: \tTotal Reward = 11.00, \tTotal TD Error = 12.9850\n",
      "SARSA Episode 2000: \tTotal Reward = 9.00, \tTotal TD Error = 9.3616\n",
      "SARSA Episode 3000: \tTotal Reward = 22.00, \tTotal TD Error = 50.6003\n",
      "SARSA Episode 4000: \tTotal Reward = 65.00, \tTotal TD Error = 63.8202\n",
      "SARSA Episode 5000: \tTotal Reward = 20.00, \tTotal TD Error = 20.8382\n",
      "SARSA Episode 6000: \tTotal Reward = 69.00, \tTotal TD Error = 69.7341\n",
      "SARSA Episode 7000: \tTotal Reward = 31.00, \tTotal TD Error = 30.5258\n",
      "SARSA Episode 8000: \tTotal Reward = 36.00, \tTotal TD Error = 37.2873\n",
      "SARSA Episode 9000: \tTotal Reward = 39.00, \tTotal TD Error = 40.5374\n",
      "SARSA Episode 10000: \tTotal Reward = 39.00, \tTotal TD Error = 38.5351\n",
      "SARSA Episode 11000: \tTotal Reward = 61.00, \tTotal TD Error = 59.7580\n",
      "SARSA Episode 12000: \tTotal Reward = 15.00, \tTotal TD Error = 15.2136\n",
      "SARSA Episode 13000: \tTotal Reward = 23.00, \tTotal TD Error = 22.3227\n",
      "SARSA Episode 14000: \tTotal Reward = 15.00, \tTotal TD Error = 14.1043\n",
      "SARSA Episode 15000: \tTotal Reward = 74.00, \tTotal TD Error = 73.5510\n",
      "SARSA Episode 16000: \tTotal Reward = 31.00, \tTotal TD Error = 32.8204\n",
      "SARSA Episode 17000: \tTotal Reward = 30.00, \tTotal TD Error = 30.5234\n",
      "SARSA Episode 18000: \tTotal Reward = 32.00, \tTotal TD Error = 31.1423\n",
      "SARSA Episode 19000: \tTotal Reward = 20.00, \tTotal TD Error = 19.4299\n",
      "SARSA Episode 20000: \tTotal Reward = 32.00, \tTotal TD Error = 31.1687\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Perform SARSA with linear function approximation\n",
    "\"\"\"\n",
    "print(\"Running SARSA with Linear Function Approximation...\")\n",
    "\n",
    "# Call the SARSA function to learn the Q-values\n",
    "sarsa(env, num_episodes = 20000, exploration_rate = 0.5) \n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 2.1: Temporal Difference (TD) Learning with Linear Function Approximation (Q-Learning)\n",
    "\"\"\"\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01  \n",
    "discount_factor = 0.90  \n",
    "num_features = 4            # Number of state features\n",
    "\n",
    "# Initialize weights for linear approximation\n",
    "weights_q_learning = np.zeros((env.action_space.n, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, exploration_rate):\n",
    "    \"\"\"\n",
    "    Function to perform Q-Learning with linear function approximation\n",
    "    \"\"\"\n",
    "\n",
    "    # Loop over episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        # Initialize metrics\n",
    "        total_reward = 0  # Track total reward in the episode\n",
    "        total_td_error = 0  # Track total TD error in the episode\n",
    "\n",
    "        # Loop over steps within the episode\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Choose an action using epsilon-greedy policy\n",
    "            action = choose_action_linear(weights_q_learning, state, exploration_rate)\n",
    "\n",
    "            # Take the action and observe the next state and reward\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            # Compute Q-value for the current state and action\n",
    "            features = extract_features(state)\n",
    "            q_value = np.dot(weights_q_learning[action], features)\n",
    "\n",
    "            # Compute the Q-value for the next state\n",
    "            next_features = extract_features(next_state)\n",
    "            next_q_values = [np.dot(weights_q_learning[a], next_features) for a in range(env.action_space.n)]\n",
    "\n",
    "            # Compute the TD error\n",
    "            td_error = np.add(reward, np.multiply(discount_factor, np.max(next_q_values))) - q_value\n",
    "\n",
    "            # Update the weights using the TD error\n",
    "            weights_q_learning[action] = np.add(weights_q_learning[action], np.multiply(learning_rate * td_error, features))\n",
    "\n",
    "            # Accumulate reward and TD error\n",
    "            total_reward += reward\n",
    "            total_td_error += abs(td_error)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "\n",
    "        # Print metrics every 500 episodes\n",
    "        if (episode + 1) % 500 == 0:\n",
    "            print(f\"Q-Learning Episode {episode + 1}: \\tTotal Reward = {total_reward:.2f}, \"\n",
    "                  f\"\\tTotal TD Error = {total_td_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Q-Learning with Linear Function Approximation...\n",
      "Q-Learning Episode 500: \tTotal Reward = 9.00, \tTotal TD Error = 16.3235\n",
      "Q-Learning Episode 1000: \tTotal Reward = 10.00, \tTotal TD Error = 22.9546\n",
      "Q-Learning Episode 1500: \tTotal Reward = 9.00, \tTotal TD Error = 16.2150\n",
      "Q-Learning Episode 2000: \tTotal Reward = 9.00, \tTotal TD Error = 12.8818\n",
      "Q-Learning Episode 2500: \tTotal Reward = 10.00, \tTotal TD Error = 30.9727\n",
      "Q-Learning Episode 3000: \tTotal Reward = 9.00, \tTotal TD Error = 65.9485\n",
      "Q-Learning Episode 3500: \tTotal Reward = 10.00, \tTotal TD Error = 121.7913\n",
      "Q-Learning Episode 4000: \tTotal Reward = 500.00, \tTotal TD Error = 744.7463\n",
      "Q-Learning Episode 4500: \tTotal Reward = 486.00, \tTotal TD Error = 541.1092\n",
      "Q-Learning Episode 5000: \tTotal Reward = 500.00, \tTotal TD Error = 520.4110\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Perform Q-Learning with linear function approximation\n",
    "\"\"\"\n",
    "print(\"\\nRunning Q-Learning with Linear Function Approximation...\")\n",
    "\n",
    "# Call the Q-Learning function to learn the Q-values\n",
    "q_learning(env, num_episodes = 5000, exploration_rate = 0.2)\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 2.2: Temporal Difference (TD) Learning with Deep Q-Network (DQN)\n",
    "\"\"\"\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network (DQN) class\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Constructor for QNetwork class\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)   # Input to hidden layer 1\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)  # Hidden layer 1 to hidden layer 2\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)  # Hidden layer 2 to output\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network\n",
    "        \"\"\"\n",
    "        x = torch.relu(self.fc1(x))     # ReLU activation for hidden layer 1\n",
    "        x = torch.relu(self.fc2(x))     # ReLU activation for hidden layer 2\n",
    "        x = self.fc3(x)                 # Output layer (raw Q-values)\n",
    "        return x\n",
    "    \n",
    "def choose_action_dqn(network, state, exploration_rate):\n",
    "    \"\"\"\n",
    "    Function to choose an action using epsilon-greedy policy\n",
    "    \"\"\"\n",
    "    if np.random.rand() < exploration_rate:\n",
    "        # Choose a random action\n",
    "        return np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        q_values = network(state).cpu().detach().numpy()\n",
    "        return np.argmax(q_values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialize the DQN model for SARSA and Q-Learning\n",
    "\"\"\"\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = env.observation_space.shape[0]     # State size (4 for CartPole)\n",
    "hidden_size = 64                                # Number of neurons in the hidden layers\n",
    "output_size = env.action_space.n                # Number of actions (2 for CartPole)\n",
    "learning_rate = 0.00001\n",
    "min_exploration_rate = 0.01\n",
    "\n",
    "# Initialize the DQN model\n",
    "q_network_sarsa = QNetwork(input_size, hidden_size, output_size)\n",
    "q_network_q_learning = QNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer_sarsa = optim.Adam(q_network_sarsa.parameters(), lr=learning_rate)\n",
    "optimizer_q_learning = optim.Adam(q_network_q_learning.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_dqn(env, num_episodes, exploration_rate, exploration_decay):\n",
    "    \"\"\"\n",
    "    Function to perform SARSA with DQN\n",
    "    \"\"\"\n",
    "\n",
    "    # Loop over episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment and choose the initial action\n",
    "        state, _ = env.reset()\n",
    "        action = choose_action_dqn(q_network_sarsa, state, exploration_rate)\n",
    "        total_reward = 0\n",
    "        total_td_error = 0\n",
    "\n",
    "        # Loop over steps within the episode\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Take a step in the environment and choose the next action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_action = choose_action_dqn(q_network_sarsa, next_state, exploration_rate)\n",
    "\n",
    "            # Compute target Q-value and predicted Q-value\n",
    "            target_q_value = reward + discount_factor * q_network_sarsa(\n",
    "                torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            )[0][next_action].item()\n",
    "            predicted_q_value = q_network_sarsa(torch.FloatTensor(state).unsqueeze(0))[0][action]\n",
    "\n",
    "            # Compute TD error\n",
    "            td_error = target_q_value - predicted_q_value.item()\n",
    "            \n",
    "            # Update the Q-value using the TD error\n",
    "            loss = loss_fn(predicted_q_value, torch.tensor(target_q_value))\n",
    "            optimizer_sarsa.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_sarsa.step()\n",
    "\n",
    "            # Accumulate reward and TD error\n",
    "            total_reward += reward\n",
    "            total_td_error += abs(td_error)\n",
    "\n",
    "            # Move to the next state and action\n",
    "            state, action = next_state, next_action\n",
    "            done = terminated or truncated\n",
    "\n",
    "        # Decay exploration rate\n",
    "        exploration_rate = max(min_exploration_rate, exploration_rate * exploration_decay)\n",
    "\n",
    "        # Print metrics every 500 episodes\n",
    "        if (episode + 1) % 500 == 0:\n",
    "            print(f\"SARSA Episode {episode + 1}: \\tTotal Reward = {total_reward}, \"\n",
    "                  f\"\\tTotal TD Error = {total_td_error:.4f}, \\tExploration Rate = {exploration_rate:.4f}\")# Training parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running SARSA with Deep Q-Network Approximation...\n",
      "SARSA Episode 500: \tTotal Reward = 11.0, \tTotal TD Error = 19.3427, \tExploration Rate = 0.6064\n",
      "SARSA Episode 1000: \tTotal Reward = 11.0, \tTotal TD Error = 32.3000, \tExploration Rate = 0.3677\n",
      "SARSA Episode 1500: \tTotal Reward = 14.0, \tTotal TD Error = 68.3480, \tExploration Rate = 0.2230\n",
      "SARSA Episode 2000: \tTotal Reward = 9.0, \tTotal TD Error = 65.5374, \tExploration Rate = 0.1352\n",
      "SARSA Episode 2500: \tTotal Reward = 10.0, \tTotal TD Error = 105.1397, \tExploration Rate = 0.0820\n",
      "SARSA Episode 3000: \tTotal Reward = 10.0, \tTotal TD Error = 151.2536, \tExploration Rate = 0.0497\n",
      "SARSA Episode 3500: \tTotal Reward = 8.0, \tTotal TD Error = 178.5381, \tExploration Rate = 0.0301\n",
      "SARSA Episode 4000: \tTotal Reward = 10.0, \tTotal TD Error = 310.5276, \tExploration Rate = 0.0183\n",
      "SARSA Episode 4500: \tTotal Reward = 9.0, \tTotal TD Error = 394.0682, \tExploration Rate = 0.0111\n",
      "SARSA Episode 5000: \tTotal Reward = 8.0, \tTotal TD Error = 475.6613, \tExploration Rate = 0.0100\n"
     ]
    }
   ],
   "source": [
    "# Training parameters for SARSA with DQN\n",
    "num_episodes = 5000\n",
    "initial_exploration_rate = 1.0      # Initial exploration rate\n",
    "exploration_decay = 0.999           # Exploration decay rate\n",
    "min_exploration_rate = 0.01         # Minimum exploration rate\n",
    "discount_factor = 0.99              # Discount factor\n",
    "learning_rate = 0.00001             # Learning rate\n",
    "\n",
    "# Initialize the DQN model\n",
    "print(\"\\nRunning SARSA with Deep Q-Network Approximation...\")\n",
    "sarsa_dqn(env, num_episodes, initial_exploration_rate, exploration_decay)\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_dqn(env, num_episodes, exploration_rate, exploration_decay):\n",
    "    \"\"\"\n",
    "    Function to perform Q-Learning with DQN\n",
    "    \"\"\"\n",
    "    discount_factor = 0.99  # Set your discount factor here\n",
    "    \n",
    "    # Loop over episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment and choose the initial action\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        total_td_error = 0\n",
    "\n",
    "        # Loop over steps within the episode\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Choose the action\n",
    "            action = choose_action_dqn(q_network_q_learning, state, exploration_rate)\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            # Compute target Q-value using max Q-value for the next state (Q-learning update)\n",
    "            next_q_values = q_network_q_learning(torch.FloatTensor(next_state).unsqueeze(0))\n",
    "            max_next_q_value = torch.max(next_q_values).item()\n",
    "\n",
    "            target_q_value = reward + discount_factor * max_next_q_value\n",
    "            predicted_q_value = q_network_q_learning(torch.FloatTensor(state).unsqueeze(0))[0][action]\n",
    "\n",
    "            # Compute TD error\n",
    "            td_error = target_q_value - predicted_q_value.item()\n",
    "\n",
    "            # Update the Q-value using the TD error\n",
    "            loss = loss_fn(predicted_q_value, torch.tensor(target_q_value))\n",
    "            optimizer_q_learning.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_q_learning.step()\n",
    "\n",
    "            # Accumulate reward and TD error\n",
    "            total_reward += reward\n",
    "            total_td_error += abs(td_error)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "\n",
    "        # Decay exploration rate\n",
    "        exploration_rate = max(min_exploration_rate, exploration_rate * exploration_decay)\n",
    "\n",
    "        if (episode + 1) % 500 == 0:\n",
    "            print(f\"Q-Learning Episode {episode + 1}: \\tTotal Reward = {total_reward}, \"\n",
    "                  f\"\\tTotal TD Error = {total_td_error:.4f}, \\tExploration Rate = {exploration_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Q-Learning with Deep Q-Network Approximation...\n",
      "Q-Learning Episode 500: \tTotal Reward = 21.0, \tTotal TD Error = 11156.4584, \tExploration Rate = 0.6064\n",
      "Q-Learning Episode 1000: \tTotal Reward = 9.0, \tTotal TD Error = 6629.9751, \tExploration Rate = 0.3677\n",
      "Q-Learning Episode 1500: \tTotal Reward = 11.0, \tTotal TD Error = 6177.3909, \tExploration Rate = 0.2230\n",
      "Q-Learning Episode 2000: \tTotal Reward = 8.0, \tTotal TD Error = 5334.5876, \tExploration Rate = 0.1352\n",
      "Q-Learning Episode 2500: \tTotal Reward = 9.0, \tTotal TD Error = 11208.8033, \tExploration Rate = 0.0820\n",
      "Q-Learning Episode 3000: \tTotal Reward = 9.0, \tTotal TD Error = 7842.2059, \tExploration Rate = 0.0497\n",
      "Q-Learning Episode 3500: \tTotal Reward = 10.0, \tTotal TD Error = 9738.6653, \tExploration Rate = 0.0301\n",
      "Q-Learning Episode 4000: \tTotal Reward = 11.0, \tTotal TD Error = 12063.9040, \tExploration Rate = 0.0183\n",
      "Q-Learning Episode 4500: \tTotal Reward = 8.0, \tTotal TD Error = 10089.2021, \tExploration Rate = 0.0111\n",
      "Q-Learning Episode 5000: \tTotal Reward = 10.0, \tTotal TD Error = 13714.6859, \tExploration Rate = 0.0100\n"
     ]
    }
   ],
   "source": [
    "# Training parameters for Q-Learning with DQN\n",
    "num_episodes = 5000\n",
    "initial_exploration_rate = 1.0      # Initial exploration rate\n",
    "exploration_decay = 0.999           # Exploration decay rate\n",
    "min_exploration_rate = 0.01         # Minimum exploration rate\n",
    "discount_factor = 0.99              # Discount factor\n",
    "learning_rate = 0.00001             # Learning rate\n",
    "\n",
    "# Initialize the DQN model\n",
    "print(\"\\nRunning Q-Learning with Deep Q-Network Approximation...\")\n",
    "q_learning_dqn(env, num_episodes, initial_exploration_rate, exploration_decay)\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
