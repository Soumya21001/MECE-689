{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /Users/dd/anaconda3/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/dd/anaconda3/lib/python3.11/site-packages (from gymnasium) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/dd/anaconda3/lib/python3.11/site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/dd/anaconda3/lib/python3.11/site-packages (from gymnasium) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/dd/anaconda3/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: False\n",
      "Tensor is on device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check if GPU is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f\"GPU Available: {gpu_available}\")\n",
    "\n",
    "# If GPU is available, print the GPU name\n",
    "if gpu_available:\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Check if tensors are being moved to GPU\n",
    "device = torch.device(\"cuda\" if gpu_available else \"cpu\")\n",
    "tensor = torch.rand(3, 3).to(device)\n",
    "print(f\"Tensor is on device: {tensor.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CartPole-v1 environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Reset the environment to obtain the initial state\n",
    "state, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 1.1: State Discretization\n",
    "\"\"\"\n",
    "\n",
    "# Define the number of bins for each state component\n",
    "num_bins = 10\n",
    "\n",
    "# Define bin ranges for each state component based on typical CartPole state ranges\n",
    "cart_position_range = [-2.4, 2.4]\n",
    "cart_velocity_range = [-2.0, 2.0]\n",
    "pole_angle_range = [-0.2095, 0.2095]  # Roughly Â±12 degrees in radians\n",
    "pole_velocity_range = [-3.0, 3.0]\n",
    "\n",
    "# Custom mapping function to discretize continuous state values\n",
    "def custom_discretize(value, value_range, num_bins):\n",
    "    \"\"\"\n",
    "    Discretize a continuous value using the provided value range and number of bins.\n",
    "    \"\"\"\n",
    "    # Extract the min and max values from the value range\n",
    "    min_val, max_val = value_range\n",
    "\n",
    "    # Check if value is below min range or above max range, and clamp it\n",
    "    if value <= min_val:\n",
    "        return 0\n",
    "    elif value >= max_val:\n",
    "        return num_bins - 1\n",
    "\n",
    "    # Calculate the relative position of value within its range\n",
    "    bin_width = (max_val - min_val) / num_bins\n",
    "    bin_index = int((value - min_val) / bin_width)\n",
    "    return bin_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(state):\n",
    "    \"\"\"\n",
    "    Discretize a state into a tuple of bin indices based on the bin ranges and number of bins defined above.\n",
    "    \"\"\"\n",
    "    # Unpack the state components\n",
    "    cart_pos, cart_vel, pole_angle, pole_vel = state\n",
    "\n",
    "    # Map each component of the state to a discrete bin index using the custom function\n",
    "    cart_pos_discrete = custom_discretize(cart_pos, cart_position_range, num_bins)\n",
    "    cart_vel_discrete = custom_discretize(cart_vel, cart_velocity_range, num_bins)\n",
    "    pole_angle_discrete = custom_discretize(pole_angle, pole_angle_range, num_bins)\n",
    "    pole_vel_discrete = custom_discretize(pole_vel, pole_velocity_range, num_bins)\n",
    "\n",
    "    # Return the discrete state as a tuple of bin indices\n",
    "    return (cart_pos_discrete, cart_vel_discrete, pole_angle_discrete, pole_vel_discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continuous_states [[ 0.02642061  0.02536744  0.00326753 -0.02269654]\n",
      " [ 0.02692796 -0.16980122  0.0028136   0.27101552]\n",
      " [ 0.02353194  0.02528047  0.00823391 -0.02077864]\n",
      " ...\n",
      " [-0.03472318 -0.833149    0.15159914  1.3649452 ]\n",
      " [-0.05138616 -0.6402155   0.17889804  1.1232638 ]\n",
      " [-0.06419047 -0.8371734   0.20136331  1.4663014 ]]\n",
      "discretized_states [[5 5 5 4]\n",
      " [5 4 5 5]\n",
      " [5 5 5 4]\n",
      " ...\n",
      " [4 2 8 7]\n",
      " [4 3 9 6]\n",
      " [4 2 9 7]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Discretize the initial state\n",
    "\"\"\"\n",
    "\n",
    "# Gather states and their discretized versions\n",
    "continuous_states = []\n",
    "discretized_states = []\n",
    "\n",
    "# Run the environment and collect state samples\n",
    "for episode in range(20):  # Collect data from 20 episodes\n",
    "    state, _ = env.reset()  # Reset environment and get initial state\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Save continuous state for plotting\n",
    "        continuous_states.append(state)\n",
    "        \n",
    "        # Discretize the state and save it\n",
    "        discretized_states.append(discretize_state(state))\n",
    "        \n",
    "        # Take a random action and step through the environment\n",
    "        action = env.action_space.sample()\n",
    "        state, _, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        # Check if the episode is over\n",
    "        done = terminated or truncated\n",
    "\n",
    "# Convert the collected states to NumPy arrays for easy manipulation\n",
    "continuous_states = np.array(continuous_states)\n",
    "discretized_states = np.array(discretized_states)\n",
    "\n",
    "# Print the continuous and discretized states\n",
    "print('continuous_states', continuous_states)\n",
    "print('discretized_states', discretized_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 1.3: Implementing Temporal Difference Learning - SARSA & Q-Learning\n",
    "\"\"\"\n",
    "\n",
    "# Define the action space and Q-table dimensions\n",
    "num_actions = env.action_space.n  # Number of actions in CartPole (left, right)\n",
    "state_space_size = (num_bins,) * 4  # 4-tuple of number of bins for each state component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(action, state, exploration_rate=0.5):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy policy with a small chance of exploration.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < exploration_rate:\n",
    "        # Explore: Choose a random action\n",
    "        return np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Exploit: Choose the action with the highest Q-value\n",
    "        return np.argmax(action[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_control(env, num_episodes, Q_table, exploration_rate, learning_rate, discount_factor, method):\n",
    "    \"\"\"\n",
    "    Temporal Difference Learning algorithm for Q-Learning and SARSA.\n",
    "    \"\"\"\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment and discretize the initial state\n",
    "        state, _ = env.reset()\n",
    "        discrete_state = discretize_state(state)\n",
    "\n",
    "        total_reward = 0  # Track total reward in each episode\n",
    "        total_td_error = 0  # Track total TD error in each episode\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Choose the action using the epsilon-greedy policy\n",
    "            action = choose_action(Q_table, discrete_state, exploration_rate)\n",
    "\n",
    "            # Take the chosen action and observe the next state and reward\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_discrete_state = discretize_state(next_state)\n",
    "\n",
    "            if method == \"sarsa\":\n",
    "                # SARSA: Choose the next action using the current policy (on-policy)\n",
    "                next_action = choose_action(Q_table, next_discrete_state, exploration_rate)\n",
    "                td_target = reward + discount_factor * Q_table[next_discrete_state][next_action]\n",
    "\n",
    "            elif method == \"q_learning\":\n",
    "                # Q-Learning: Use the greedy action for the next state (off-policy)\n",
    "                best_next_action = np.argmax(Q_table[next_discrete_state])\n",
    "                td_target = reward + discount_factor * Q_table[next_discrete_state][best_next_action]\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Method must be 'sarsa' or 'q_learning'\")\n",
    "\n",
    "            # Calculate the TD error and update the Q-value\n",
    "            td_error = td_target - Q_table[discrete_state][action]\n",
    "            Q_table[discrete_state][action] += learning_rate * td_error\n",
    "\n",
    "            # Accumulate total reward and TD error\n",
    "            total_reward += reward\n",
    "            total_td_error += abs(td_error)\n",
    "\n",
    "            # Move to the next state\n",
    "            discrete_state = next_discrete_state\n",
    "\n",
    "            # Check if episode is done\n",
    "            done = terminated or truncated\n",
    "\n",
    "        # Print metrics every 1000 episodes\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            print(f\"Episode {episode + 1}: \\tTotal Reward = {total_reward:.2f}, \"\n",
    "                  f\"\\tTotal TD Error = {total_td_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000: \tTotal Reward = 11.00, \tTotal TD Error = 16.8553\n",
      "Episode 2000: \tTotal Reward = 9.00, \tTotal TD Error = 16.6455\n",
      "Episode 3000: \tTotal Reward = 9.00, \tTotal TD Error = 14.6691\n",
      "Episode 4000: \tTotal Reward = 9.00, \tTotal TD Error = 20.3293\n",
      "Episode 5000: \tTotal Reward = 16.00, \tTotal TD Error = 30.9929\n",
      "Episode 6000: \tTotal Reward = 9.00, \tTotal TD Error = 9.8529\n",
      "Episode 7000: \tTotal Reward = 9.00, \tTotal TD Error = 12.0819\n",
      "Episode 8000: \tTotal Reward = 9.00, \tTotal TD Error = 21.2218\n",
      "Episode 9000: \tTotal Reward = 11.00, \tTotal TD Error = 15.0077\n",
      "Episode 10000: \tTotal Reward = 10.00, \tTotal TD Error = 13.3733\n",
      "Episode 11000: \tTotal Reward = 9.00, \tTotal TD Error = 15.5492\n",
      "Episode 12000: \tTotal Reward = 10.00, \tTotal TD Error = 37.3264\n",
      "Episode 13000: \tTotal Reward = 10.00, \tTotal TD Error = 20.4622\n",
      "Episode 14000: \tTotal Reward = 10.00, \tTotal TD Error = 11.7133\n",
      "Episode 15000: \tTotal Reward = 8.00, \tTotal TD Error = 20.9342\n",
      "Episode 16000: \tTotal Reward = 10.00, \tTotal TD Error = 11.6048\n",
      "Episode 17000: \tTotal Reward = 10.00, \tTotal TD Error = 33.9306\n",
      "Episode 18000: \tTotal Reward = 15.00, \tTotal TD Error = 134.4825\n",
      "Episode 19000: \tTotal Reward = 11.00, \tTotal TD Error = 22.9145\n",
      "Episode 20000: \tTotal Reward = 25.00, \tTotal TD Error = 30.7224\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implementing Temporal Difference Learning - Expected SARSA\n",
    "\"\"\"\n",
    "# Define necessary parameters\n",
    "num_episodes = 20000\n",
    "exploration_rate = 0.1  # Epsilon for epsilon-greedy strategy\n",
    "learning_rate = 0.01  # Alpha, step size for Q-value updates\n",
    "discount_factor = 0.99  # Gamma, discount factor for future rewards\n",
    "\n",
    "# Initialize the Q-table for Expected SARSA\n",
    "Q_sarsa = np.zeros((num_bins, num_bins, num_bins, num_bins, num_actions))\n",
    "\n",
    "# Call the td_control function for SARSA\n",
    "td_control(env, num_episodes, Q_sarsa, exploration_rate, learning_rate, discount_factor, method=\"sarsa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000: \tTotal Reward = 8.00, \tTotal TD Error = 11.3145\n",
      "Episode 2000: \tTotal Reward = 10.00, \tTotal TD Error = 19.6079\n",
      "Episode 3000: \tTotal Reward = 9.00, \tTotal TD Error = 22.1948\n",
      "Episode 4000: \tTotal Reward = 10.00, \tTotal TD Error = 13.0170\n",
      "Episode 5000: \tTotal Reward = 10.00, \tTotal TD Error = 13.6860\n",
      "Episode 6000: \tTotal Reward = 9.00, \tTotal TD Error = 13.0838\n",
      "Episode 7000: \tTotal Reward = 10.00, \tTotal TD Error = 15.4054\n",
      "Episode 8000: \tTotal Reward = 10.00, \tTotal TD Error = 13.4970\n",
      "Episode 9000: \tTotal Reward = 10.00, \tTotal TD Error = 20.9820\n",
      "Episode 10000: \tTotal Reward = 10.00, \tTotal TD Error = 16.3825\n",
      "Episode 11000: \tTotal Reward = 8.00, \tTotal TD Error = 24.1020\n",
      "Episode 12000: \tTotal Reward = 10.00, \tTotal TD Error = 17.4058\n",
      "Episode 13000: \tTotal Reward = 17.00, \tTotal TD Error = 86.6202\n",
      "Episode 14000: \tTotal Reward = 9.00, \tTotal TD Error = 12.4197\n",
      "Episode 15000: \tTotal Reward = 10.00, \tTotal TD Error = 17.6879\n",
      "Episode 16000: \tTotal Reward = 9.00, \tTotal TD Error = 16.6087\n",
      "Episode 17000: \tTotal Reward = 10.00, \tTotal TD Error = 34.3689\n",
      "Episode 18000: \tTotal Reward = 13.00, \tTotal TD Error = 54.4140\n",
      "Episode 19000: \tTotal Reward = 11.00, \tTotal TD Error = 103.1875\n",
      "Episode 20000: \tTotal Reward = 10.00, \tTotal TD Error = 12.4016\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implementing Temporal Difference Learning - Q-Learning\n",
    "\"\"\"\n",
    "# Define necessary parameters\n",
    "num_episodes = 20000\n",
    "exploration_rate = 0.1  # Epsilon for epsilon-greedy strategy\n",
    "learning_rate = 0.01  # Alpha, step size for Q-value updates\n",
    "discount_factor = 0.99  # Gamma, discount factor for future rewards\n",
    "\n",
    "# Initialize the Q-table for Q-Learning\n",
    "Q_qlearning = np.zeros((num_bins, num_bins, num_bins, num_bins, num_actions))\n",
    "\n",
    "# Call the td_control function for Q-Learning\n",
    "td_control(env, num_episodes, Q_qlearning, exploration_rate, learning_rate, discount_factor, method=\"q_learning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 1.4: Implementing Monte Carlo Prediction\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the value function and state counts to zeros using numpy arrays\n",
    "V = np.zeros((num_bins, num_bins, num_bins, num_bins))\n",
    "state_count = np.zeros((num_bins, num_bins, num_bins, num_bins))\n",
    "\n",
    "# Hyperparameters\n",
    "discount_factor = 0.99      # Discount factor for calculating returns\n",
    "num_episodes = 10000        # Number of episodes to run for MC prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_prediction(env, num_episodes, discount_factor):\n",
    "    \"\"\"\n",
    "    Perform first-visit Monte Carlo prediction to estimate the state value function\n",
    "    \"\"\"\n",
    "    # Loop over the episodes\n",
    "    for episode_num in range(num_episodes):\n",
    "        # Generate an episode using the random policy\n",
    "        episode = []  # List to store (state, reward) tuples\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        # Track states that have been visited in the episode for first-visit MC\n",
    "        while not done:\n",
    "            # Random action policy\n",
    "            discrete_state = discretize_state(state)\n",
    "            action = choose_action(V, discrete_state, exploration_rate)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            # Discretize the state and store it along with the reward in the episode\n",
    "            episode.append((discrete_state, reward))\n",
    "            \n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        # Track states that have been visited in the episode for first-visit MC\n",
    "        visited_states = set()\n",
    "        \n",
    "        # Calculate returns for each state in the episode\n",
    "        G = 0  # Return value, cumulative discounted reward\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, reward = episode[t]\n",
    "            G = discount_factor * G + reward  # Calculate return G_t\n",
    "\n",
    "            # First-visit MC: Check if this is the first visit to the state in the episode\n",
    "            if state not in visited_states:\n",
    "                visited_states.add(state)\n",
    "                \n",
    "                # Update the state value estimate using incremental formula\n",
    "                state_count[state] += 1\n",
    "                V[state] += (G - V[state]) / state_count[state]  # Incremental mean update formula\n",
    "\n",
    "        # Print progress every 50 episodes\n",
    "        if (episode_num + 1) % 1000 == 0:\n",
    "            print(f\"Episode {episode_num + 1}/{num_episodes} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000/10000 completed.\n",
      "Episode 2000/10000 completed.\n",
      "Episode 3000/10000 completed.\n",
      "Episode 4000/10000 completed.\n",
      "Episode 5000/10000 completed.\n",
      "Episode 6000/10000 completed.\n",
      "Episode 7000/10000 completed.\n",
      "Episode 8000/10000 completed.\n",
      "Episode 9000/10000 completed.\n",
      "Episode 10000/10000 completed.\n",
      "State (1, 1, 1, 1) - Estimated Value/Return: 0.00\n",
      "State (2, 2, 2, 2) - Estimated Value/Return: 0.00\n",
      "State (3, 3, 3, 3) - Estimated Value/Return: 0.00\n",
      "State (4, 4, 4, 4) - Estimated Value/Return: 9.89\n",
      "State (5, 5, 5, 5) - Estimated Value/Return: 8.74\n",
      "State (6, 6, 6, 6) - Estimated Value/Return: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Run Monte Carlo Prediction\n",
    "monte_carlo_prediction(env, num_episodes, discount_factor)\n",
    "\n",
    "# Print final state value estimates for some sample states\n",
    "sample_states = [(1,1,1,1),(2,2,2,2),(3,3,3,3),(4,4,4,4),(5,5,5,5),(6,6,6,6)]\n",
    "for state in sample_states:\n",
    "    print(f\"State {state} - Estimated Value/Return: {V[state]:.2f}\")\n",
    "\n",
    "# Close the environment after training\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
