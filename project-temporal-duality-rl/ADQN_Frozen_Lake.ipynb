{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the FrozenLake environment\n",
    "random.seed(21)\n",
    "np.random.seed(21)\n",
    "torch.manual_seed(21)\n",
    "env = gym.make(\"FrozenLake-v1\", desc=generate_random_map(size=5), is_slippery=False)\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "env.action_space.seed(21)\n",
    "env.observation_space.seed(21)  # Seed the observation space\n",
    "env.reset(seed=21)\n",
    "\n",
    "# Parameters\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.999\n",
    "batch_size = 64\n",
    "buffer_size = 10000\n",
    "target_update_freq = 10\n",
    "num_episodes = 500\n",
    "history_size = 15 # Number of past states to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, history_size):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.history_size = history_size\n",
    "\n",
    "    def add(self, transition):\n",
    "        self.buffer.append(transition) #adding a transition to buffer\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)  #Sample a batch of transitions uniformly\n",
    "        actions, rewards, next_states, dones = zip(*[transition[1:] for transition in batch])\n",
    "\n",
    "        state_histories, reward_histories = [], []\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            state_history, reward_history = self._get_history(idx)\n",
    "            state_histories.append(state_history)\n",
    "            reward_histories.append(reward_history)\n",
    "\n",
    "        return (np.array(state_histories),  # Histories of states\n",
    "                np.array(actions),          # Actions\n",
    "                np.array(rewards),          # Current rewards\n",
    "                np.array(next_states),      # Next states\n",
    "                np.array(dones),            # Done flags\n",
    "                np.array(reward_histories)) # Histories of rewards\n",
    "    \n",
    "\n",
    "    def _get_history(self, index):\n",
    "        state_history = []\n",
    "        reward_history = []\n",
    "\n",
    "        for i in range(self.history_size):\n",
    "            if index - i >= 0:\n",
    "                state_history.insert(0, self.buffer[index - i][0]) \n",
    "                reward_history.insert(0, self.buffer[index - i][2])  \n",
    "            else:\n",
    "                state_history.insert(0, np.zeros_like(self.buffer[0][0]))\n",
    "                reward_history.insert(0, 0)\n",
    "\n",
    "        return np.array(state_history), np.array(reward_history)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size, history_size) #Calling the replaybuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting state to one-hot encoding\n",
    "def one_hot_state(state, num_states):\n",
    "    one_hot = np.zeros(num_states)\n",
    "    one_hot[state] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_reward(state, action, reward, done, goal_state, visited_states):\n",
    "    if int(state.argmax()) == int(goal_state.argmax()):\n",
    "        return reward+100  # High reward for reaching the goal\n",
    "    elif done:\n",
    "        return -20  # Heavy penalty for falling\n",
    "    if tuple(state) in visited_states:\n",
    "        return reward-5   # Penalty for revisiting a state (looping)\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mechanism\n",
    "class AttentionMechanism(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(AttentionMechanism, self).__init__()\n",
    "        self.key_layer = nn.Linear(state_dim, hidden_dim)\n",
    "        self.query_layer = nn.Linear(state_dim, hidden_dim)\n",
    "        self.value_layer = nn.Linear(1 , hidden_dim)\n",
    "\n",
    "    def forward(self, query, keys, values):\n",
    "        query = self.query_layer(query).unsqueeze(1)  # (batch_size, 1, hidden_dim)\n",
    "        keys = self.key_layer(keys)                   # (batch_size, seq_len, hidden_dim)\n",
    "        values = self.value_layer(values)             # (batch_size, seq_len, hidden_dim)  \n",
    "\n",
    "        # Computing attention scores\n",
    "        scores = torch.bmm(query, keys.transpose(1, 2)/ (keys.size(-1) ** 0.5))  # Dot product - relevance score # (batch_size, 1, seq_len)\n",
    "        \n",
    "\n",
    "        include_rewards = values[..., 0]   # Remove last dim, shape: (batch_size, seq_len)\n",
    "        \n",
    "        include_rewards = include_rewards/ (include_rewards.norm(dim=-1, keepdim=True) + 1e-8)  # Normalize\n",
    "        include_rewards= include_rewards.unsqueeze(1)\n",
    "        scores = scores + include_rewards  # Shape: (batch_size, 1, seq_len)\n",
    "\n",
    "        attention_weights = torch.softmax(scores, dim=-1)  # (batch_size, 1, seq_len)\n",
    "\n",
    "        context = torch.bmm(attention_weights, values).squeeze(1)  # Weighted sum  # (batch_size, hidden_dim)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN with Attention\n",
    "class AttentionDQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(AttentionDQN, self).__init__()\n",
    "        self.attention = AttentionMechanism(state_dim, hidden_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim))\n",
    "\n",
    "    def forward(self, current_state, history, rewards):\n",
    "        \"\"\"\n",
    "        current_state: Current state (batch_size, state_dim)\n",
    "        history: Past states (batch_size, seq_len, state_dim)\n",
    "        rewards: Past rewards (batch_size, seq_len, 1)\n",
    "        \"\"\"\n",
    "        context = self.attention(current_state, history, rewards)  # Applying attention\n",
    "        combined = torch.cat([current_state, context], dim=-1)  # Combine context with current state\n",
    "        return self.fc(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = AttentionDQN(state_space, action_space)\n",
    "target_net = AttentionDQN(state_space, action_space)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: -62.0, Steps: 13, Goal Reached: No\n",
      "Episode 2, Total Reward: -24.0, Steps: 3, Goal Reached: No\n",
      "Episode 3, Total Reward: -46.0, Steps: 11, Goal Reached: No\n",
      "Episode 4, Total Reward: 9.0, Steps: 35, Goal Reached: Yes\n",
      "Episode 5, Total Reward: -24.0, Steps: 3, Goal Reached: No\n",
      "Episode 6, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 7, Total Reward: 83.0, Steps: 13, Goal Reached: Yes\n",
      "Episode 8, Total Reward: -74.0, Steps: 19, Goal Reached: No\n",
      "Episode 9, Total Reward: -32.0, Steps: 13, Goal Reached: No\n",
      "Episode 10, Total Reward: -22.0, Steps: 5, Goal Reached: No\n",
      "Episode 11, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 12, Total Reward: -24.0, Steps: 3, Goal Reached: No\n",
      "Episode 13, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 14, Total Reward: -27.0, Steps: 12, Goal Reached: No\n",
      "Episode 15, Total Reward: -17, Steps: 4, Goal Reached: No\n",
      "Episode 16, Total Reward: -33.0, Steps: 6, Goal Reached: No\n",
      "Episode 17, Total Reward: -331.0, Steps: 80, Goal Reached: No\n",
      "Episode 18, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 19, Total Reward: -52.0, Steps: 17, Goal Reached: No\n",
      "Episode 20, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 21, Total Reward: -42.0, Steps: 9, Goal Reached: No\n",
      "Episode 22, Total Reward: -169.0, Steps: 44, Goal Reached: No\n",
      "Episode 23, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 24, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 25, Total Reward: -110.0, Steps: 25, Goal Reached: No\n",
      "Episode 26, Total Reward: -38.0, Steps: 7, Goal Reached: No\n",
      "Episode 27, Total Reward: -60.0, Steps: 15, Goal Reached: No\n",
      "Episode 28, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 29, Total Reward: -80.0, Steps: 19, Goal Reached: No\n",
      "Episode 30, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 31, Total Reward: 85.0, Steps: 15, Goal Reached: Yes\n",
      "Episode 32, Total Reward: -17, Steps: 4, Goal Reached: No\n",
      "Episode 33, Total Reward: -18, Steps: 3, Goal Reached: No\n",
      "Episode 34, Total Reward: -23.0, Steps: 4, Goal Reached: No\n",
      "Episode 35, Total Reward: -84.0, Steps: 27, Goal Reached: No\n",
      "Episode 36, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 37, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 38, Total Reward: -17, Steps: 4, Goal Reached: No\n",
      "Episode 39, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 40, Total Reward: 22.0, Steps: 30, Goal Reached: Yes\n",
      "Episode 41, Total Reward: -32.0, Steps: 7, Goal Reached: No\n",
      "Episode 42, Total Reward: -18, Steps: 3, Goal Reached: No\n",
      "Episode 43, Total Reward: -204.0, Steps: 51, Goal Reached: No\n",
      "Episode 44, Total Reward: -35.0, Steps: 10, Goal Reached: No\n",
      "Episode 45, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 46, Total Reward: -43.0, Steps: 14, Goal Reached: No\n",
      "Episode 47, Total Reward: -34.0, Steps: 5, Goal Reached: No\n",
      "Episode 48, Total Reward: -65.0, Steps: 16, Goal Reached: No\n",
      "Episode 49, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 50, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 51, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 52, Total Reward: -151.0, Steps: 44, Goal Reached: No\n",
      "Episode 53, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 54, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 55, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 56, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 57, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 58, Total Reward: -24.0, Steps: 3, Goal Reached: No\n",
      "Episode 59, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 60, Total Reward: -197.0, Steps: 52, Goal Reached: No\n",
      "Episode 61, Total Reward: -273.0, Steps: 66, Goal Reached: No\n",
      "Episode 62, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 63, Total Reward: -36.0, Steps: 15, Goal Reached: No\n",
      "Episode 64, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 65, Total Reward: -23.0, Steps: 4, Goal Reached: No\n",
      "Episode 66, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 67, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 68, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 69, Total Reward: -17.0, Steps: 10, Goal Reached: No\n",
      "Episode 70, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 71, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 72, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 73, Total Reward: -128.0, Steps: 37, Goal Reached: No\n",
      "Episode 74, Total Reward: 75.0, Steps: 17, Goal Reached: Yes\n",
      "Episode 75, Total Reward: -57.0, Steps: 12, Goal Reached: No\n",
      "Episode 76, Total Reward: -24.0, Steps: 3, Goal Reached: No\n",
      "Episode 77, Total Reward: -23.0, Steps: 4, Goal Reached: No\n",
      "Episode 78, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 79, Total Reward: -34.0, Steps: 5, Goal Reached: No\n",
      "Episode 80, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 81, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 82, Total Reward: 32.0, Steps: 28, Goal Reached: Yes\n",
      "Episode 83, Total Reward: -18, Steps: 3, Goal Reached: No\n",
      "Episode 84, Total Reward: -28.0, Steps: 5, Goal Reached: No\n",
      "Episode 85, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 86, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 87, Total Reward: -45.0, Steps: 12, Goal Reached: No\n",
      "Episode 88, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 89, Total Reward: -29.0, Steps: 4, Goal Reached: No\n",
      "Episode 90, Total Reward: -65.0, Steps: 22, Goal Reached: No\n",
      "Episode 91, Total Reward: -61.0, Steps: 20, Goal Reached: No\n",
      "Episode 92, Total Reward: -49.0, Steps: 20, Goal Reached: No\n",
      "Episode 93, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 94, Total Reward: -16, Steps: 5, Goal Reached: No\n",
      "Episode 95, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 96, Total Reward: -92.0, Steps: 25, Goal Reached: No\n",
      "Episode 97, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 98, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 99, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 100, Total Reward: -44.0, Steps: 7, Goal Reached: No\n",
      "Episode 101, Total Reward: -58.0, Steps: 11, Goal Reached: No\n",
      "Episode 102, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 103, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 104, Total Reward: -24.0, Steps: 3, Goal Reached: No\n",
      "Episode 105, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 106, Total Reward: -24.0, Steps: 3, Goal Reached: No\n",
      "Episode 107, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 108, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 109, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 110, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 111, Total Reward: -21.0, Steps: 12, Goal Reached: No\n",
      "Episode 112, Total Reward: -58.0, Steps: 11, Goal Reached: No\n",
      "Episode 113, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 114, Total Reward: -29.0, Steps: 4, Goal Reached: No\n",
      "Episode 115, Total Reward: -42.0, Steps: 9, Goal Reached: No\n",
      "Episode 116, Total Reward: -27.0, Steps: 6, Goal Reached: No\n",
      "Episode 117, Total Reward: -55.0, Steps: 14, Goal Reached: No\n",
      "Episode 118, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 119, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 120, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 121, Total Reward: -74.0, Steps: 25, Goal Reached: No\n",
      "Episode 122, Total Reward: -52.0, Steps: 11, Goal Reached: No\n",
      "Episode 123, Total Reward: -32.0, Steps: 7, Goal Reached: No\n",
      "Episode 124, Total Reward: -63.0, Steps: 18, Goal Reached: No\n",
      "Episode 125, Total Reward: -29.0, Steps: 4, Goal Reached: No\n",
      "Episode 126, Total Reward: -47.0, Steps: 10, Goal Reached: No\n",
      "Episode 127, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 128, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 129, Total Reward: 32.0, Steps: 28, Goal Reached: Yes\n",
      "Episode 130, Total Reward: -34.0, Steps: 5, Goal Reached: No\n",
      "Episode 131, Total Reward: -43.0, Steps: 14, Goal Reached: No\n",
      "Episode 132, Total Reward: -16, Steps: 5, Goal Reached: No\n",
      "Episode 133, Total Reward: -29.0, Steps: 4, Goal Reached: No\n",
      "Episode 134, Total Reward: -34.0, Steps: 5, Goal Reached: No\n",
      "Episode 135, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 136, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 137, Total Reward: -33.0, Steps: 6, Goal Reached: No\n",
      "Episode 138, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 139, Total Reward: -16, Steps: 5, Goal Reached: No\n",
      "Episode 140, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 141, Total Reward: -36.0, Steps: 9, Goal Reached: No\n",
      "Episode 142, Total Reward: -28.0, Steps: 5, Goal Reached: No\n",
      "Episode 143, Total Reward: -22.0, Steps: 5, Goal Reached: No\n",
      "Episode 144, Total Reward: -23.0, Steps: 4, Goal Reached: No\n",
      "Episode 145, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 146, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 147, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 148, Total Reward: -24.0, Steps: 3, Goal Reached: No\n",
      "Episode 149, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 150, Total Reward: -29.0, Steps: 4, Goal Reached: No\n",
      "Episode 151, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 152, Total Reward: -21.0, Steps: 6, Goal Reached: No\n",
      "Episode 153, Total Reward: -24.0, Steps: 3, Goal Reached: No\n",
      "Episode 154, Total Reward: -45.0, Steps: 12, Goal Reached: No\n",
      "Episode 155, Total Reward: -17.0, Steps: 10, Goal Reached: No\n",
      "Episode 156, Total Reward: -44.0, Steps: 19, Goal Reached: No\n",
      "Episode 157, Total Reward: -45.0, Steps: 12, Goal Reached: No\n",
      "Episode 158, Total Reward: -123.0, Steps: 36, Goal Reached: No\n",
      "Episode 159, Total Reward: -24.0, Steps: 3, Goal Reached: No\n",
      "Episode 160, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 161, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 162, Total Reward: -34.0, Steps: 5, Goal Reached: No\n",
      "Episode 163, Total Reward: -17, Steps: 4, Goal Reached: No\n",
      "Episode 164, Total Reward: -69.0, Steps: 18, Goal Reached: No\n",
      "Episode 165, Total Reward: -20.0, Steps: 7, Goal Reached: No\n",
      "Episode 166, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 167, Total Reward: -30.0, Steps: 9, Goal Reached: No\n",
      "Episode 168, Total Reward: -20.0, Steps: 7, Goal Reached: No\n",
      "Episode 169, Total Reward: -14, Steps: 7, Goal Reached: No\n",
      "Episode 170, Total Reward: -42.0, Steps: 9, Goal Reached: No\n",
      "Episode 171, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 172, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 173, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 174, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 175, Total Reward: -18, Steps: 3, Goal Reached: No\n",
      "Episode 176, Total Reward: -24.0, Steps: 3, Goal Reached: No\n",
      "Episode 177, Total Reward: -17, Steps: 4, Goal Reached: No\n",
      "Episode 178, Total Reward: -16, Steps: 5, Goal Reached: No\n",
      "Episode 179, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 180, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 181, Total Reward: -17, Steps: 4, Goal Reached: No\n",
      "Episode 182, Total Reward: -39.0, Steps: 6, Goal Reached: No\n",
      "Episode 183, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 184, Total Reward: -18, Steps: 3, Goal Reached: No\n",
      "Episode 185, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 186, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 187, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 188, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 189, Total Reward: -3.0, Steps: 35, Goal Reached: Yes\n",
      "Episode 190, Total Reward: -59.0, Steps: 16, Goal Reached: No\n",
      "Episode 191, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 192, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 193, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 194, Total Reward: -34.0, Steps: 5, Goal Reached: No\n",
      "Episode 195, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 196, Total Reward: -21.0, Steps: 6, Goal Reached: No\n",
      "Episode 197, Total Reward: -58.0, Steps: 11, Goal Reached: No\n",
      "Episode 198, Total Reward: -14, Steps: 7, Goal Reached: No\n",
      "Episode 199, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 200, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 201, Total Reward: -18, Steps: 3, Goal Reached: No\n",
      "Episode 202, Total Reward: 69.0, Steps: 17, Goal Reached: Yes\n",
      "Episode 203, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 204, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 205, Total Reward: -39.0, Steps: 12, Goal Reached: No\n",
      "Episode 206, Total Reward: -19, Steps: 2, Goal Reached: No\n",
      "Episode 207, Total Reward: -39.0, Steps: 12, Goal Reached: No\n",
      "Episode 208, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 209, Total Reward: -43.0, Steps: 8, Goal Reached: No\n",
      "Episode 210, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 211, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 212, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 213, Total Reward: -33.0, Steps: 6, Goal Reached: No\n",
      "Episode 214, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 215, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Episode 216, Total Reward: -20, Steps: 1, Goal Reached: No\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     73\u001b[0m         torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(policy_net\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;66;03m#gradient clipping\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     76\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(epsilon \u001b[38;5;241m*\u001b[39m epsilon_decay, epsilon_min)  \u001b[38;5;66;03m#updating epsilon\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m target_update_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m#Updating target_net\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     adam(\n\u001b[0;32m    224\u001b[0m         params_with_grad,\n\u001b[0;32m    225\u001b[0m         grads,\n\u001b[0;32m    226\u001b[0m         exp_avgs,\n\u001b[0;32m    227\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    228\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    229\u001b[0m         state_steps,\n\u001b[0;32m    230\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    231\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    232\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    233\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    234\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    235\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    236\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    237\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    238\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    239\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    240\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    241\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    242\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    243\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    244\u001b[0m     )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m func(\n\u001b[0;32m    785\u001b[0m     params,\n\u001b[0;32m    786\u001b[0m     grads,\n\u001b[0;32m    787\u001b[0m     exp_avgs,\n\u001b[0;32m    788\u001b[0m     exp_avg_sqs,\n\u001b[0;32m    789\u001b[0m     max_exp_avg_sqs,\n\u001b[0;32m    790\u001b[0m     state_steps,\n\u001b[0;32m    791\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    792\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    793\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    794\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    795\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    796\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    797\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    798\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    799\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    800\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    801\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    802\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[0;32m    803\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Soumya Taneja\\myAnaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 432\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training Loop\n",
    "total_rewards = []\n",
    "success_count = 0\n",
    "steps_to_goal = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = one_hot_state(state, state_space)\n",
    "    history = deque(maxlen=history_size)\n",
    "    rewards = deque(maxlen=history_size)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    goal_state = state_space - 1  # Assumption - the last state corresponds to the goal\n",
    "    goal_state = one_hot_state(goal_state, state_space)\n",
    "    visited_states = set()\n",
    "\n",
    "    while not done:\n",
    "        while len(history) < history_size:  \n",
    "            history.append(np.zeros_like(state))  # Padding the history with zeros before enough steps are taken.\n",
    "            rewards.append(0)\n",
    "\n",
    "        #converting to tensors\n",
    "        history_tensor = torch.tensor(np.array(history), dtype=torch.float32).unsqueeze(0)\n",
    "        rewards_tensor = torch.tensor(np.array(rewards), dtype=torch.float32).unsqueeze(-1).unsqueeze(0)\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(range(action_space))  # Exploration\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = torch.argmax(policy_net(state_tensor, history_tensor, rewards_tensor)).item()  # Exploitation\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = one_hot_state(next_state, state_space)\n",
    "\n",
    "        reward = custom_reward(next_state, action, reward, done, goal_state, visited_states)\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    " \n",
    "        visited_states.add(tuple(next_state))  #updating visiting states for checking looping\n",
    "\n",
    "        history.append(state)  \n",
    "        rewards.append(reward)  \n",
    "        replay_buffer.add((state, action, reward, next_state, done)) # Storing transition in replay buffer\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # Training the Q-Network\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            state_histories, actions, rewards_batch, next_states, dones, reward_histories = replay_buffer.sample(batch_size)\n",
    "\n",
    "            state_histories = torch.tensor(state_histories, dtype=torch.float32)\n",
    "            reward_histories = torch.tensor(reward_histories, dtype=torch.float32).unsqueeze(-1)\n",
    "            actions = torch.tensor(actions, dtype=torch.long)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "            # Computing target Q-values\n",
    "            with torch.no_grad():\n",
    "                next_state_histories = torch.cat((state_histories[:, 1:, :], next_states.unsqueeze(1)), dim=1)\n",
    "                next_reward_histories = torch.cat((reward_histories[:, 1:, :], rewards_batch.unsqueeze(1).unsqueeze(-1)), dim=1)\n",
    "                target_q_values = rewards_batch + gamma * (1 - dones) * torch.max(target_net(next_states, next_state_histories, next_reward_histories), dim=1)[0]\n",
    "\n",
    "            # Computing current Q-values\n",
    "            current_q_values = policy_net(state_histories[:, -1, :], state_histories, reward_histories).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=1.0) #gradient clipping\n",
    "            optimizer.step()\n",
    "\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)  #updating epsilon\n",
    "\n",
    "    if episode % target_update_freq == 0:  #Updating target_net\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    total_rewards.append(total_reward)\n",
    "    if done and int(state.argmax()) == int(goal_state.argmax()):\n",
    "        success_count += 1\n",
    "        steps_to_goal.append(step_count)\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Steps: {step_count}, Goal Reached: {'Yes' if done and int(state.argmax()) == int(goal_state.argmax()) else 'No'}\")\n",
    "\n",
    "# Calculating metrics\n",
    "avg_reward = np.mean(total_rewards)\n",
    "success_rate = (success_count / num_episodes) * 100\n",
    "avg_steps = np.mean(steps_to_goal) if steps_to_goal else float('inf')\n",
    "\n",
    "print(\"\\n--- Training Metrics ---\")\n",
    "print(f\"Average Reward: {avg_reward:.2f}\")\n",
    "print(f\"Success Rate: {success_rate:.2f}%\")\n",
    "print(f\"Average Steps to Goal (for successful episodes): {avg_steps:.2f}\")\n",
    "print(\"------------------------\")\n",
    "\n",
    "# Saving the trained model\n",
    "torch.save(policy_net.state_dict(), \"model/Attention_DQN_policy_net.pth\")\n",
    "print(\"Training complete. Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Loop\n",
    "def test_attention_dqn(env, policy_net, state_space, action_space, history_size, custom_reward, num_episodes=100):\n",
    "    \n",
    "    total_rewards_test = []\n",
    "    success_count_test = 0\n",
    "    steps_to_goal_test = []\n",
    "\n",
    "    policy_net.load_state_dict(torch.load(\"model/Attention_DQN_policy_net.pth\"))\n",
    "    policy_net.eval()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = one_hot_state(state, state_space)\n",
    "        history = deque(maxlen=history_size)\n",
    "        rewards = deque(maxlen=history_size)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "        goal_state = one_hot_state(state_space - 1, state_space)\n",
    "        visited_states = set()\n",
    "\n",
    "        while not done:\n",
    "            while len(history) < history_size:\n",
    "                history.append(np.zeros_like(state))\n",
    "                rewards.append(0)\n",
    "\n",
    "            history_tensor = torch.tensor(np.array(history), dtype=torch.float32).unsqueeze(0)\n",
    "            rewards_tensor = torch.tensor(np.array(rewards), dtype=torch.float32).unsqueeze(-1).unsqueeze(0)\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            if random.random() < 0.1:  # 10% chance to explore\n",
    "                     action = random.choice(range(action_space))\n",
    "            else: \n",
    "                with torch.no_grad():\n",
    "                    action = torch.argmax(policy_net(state_tensor, history_tensor, rewards_tensor)).item()\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = one_hot_state(next_state, state_space)\n",
    "\n",
    "            reward = custom_reward(next_state, action, reward, done, goal_state, visited_states)\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "            visited_states.add(tuple(next_state))\n",
    "\n",
    "            history.append(state)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # Updating testing metrics\n",
    "        total_rewards_test.append(total_reward)\n",
    "        if done and int(state.argmax()) == int(goal_state.argmax()):\n",
    "            success_count_test += 1\n",
    "            steps_to_goal_test.append(step_count)\n",
    "\n",
    "        print(f\"Test Episode {episode + 1}, Total Reward: {total_reward}, Steps: {step_count}, Goal Reached: {'Yes' if done and int(state.argmax()) == int(goal_state.argmax()) else 'No'}\")\n",
    "\n",
    "    # Calculating test metrics\n",
    "    avg_reward_test = np.mean(total_rewards_test)\n",
    "    success_rate_test = (success_count_test / num_episodes) * 100\n",
    "    avg_steps_test = np.mean(steps_to_goal_test) if steps_to_goal_test else float('inf')\n",
    "\n",
    "    print(\"\\n--- Testing Metrics ---\")\n",
    "    print(f\"Average Reward: {avg_reward_test:.2f}\")\n",
    "    print(f\"Success Rate: {success_rate_test:.2f}%\")\n",
    "    print(f\"Average Steps to Goal (for successful episodes): {avg_steps_test:.2f}\")\n",
    "    print(\"------------------------\")\n",
    "\n",
    "    # Return metrics as a dictionary\n",
    "    metrics = {\"Average Reward\": avg_reward_test,\n",
    "        \"Success Rate (%)\": success_rate_test,\n",
    "        \"Average Steps to Goal\": avg_steps_test}\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the trained policy...\n",
      "Test Episode 1, Total Reward: -205.0, Steps: 73, Goal Reached: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soumya Taneja\\AppData\\Local\\Temp\\ipykernel_19416\\3317908633.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_net.load_state_dict(torch.load(\"model/Attention_DQN_policy_net.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 2, Total Reward: -2658.0, Steps: 566, Goal Reached: Yes\n",
      "Test Episode 3, Total Reward: -2674.0, Steps: 568, Goal Reached: Yes\n",
      "Test Episode 4, Total Reward: -1506.0, Steps: 332, Goal Reached: Yes\n",
      "Test Episode 5, Total Reward: -829.0, Steps: 199, Goal Reached: Yes\n",
      "Test Episode 6, Total Reward: -4035.0, Steps: 839, Goal Reached: Yes\n",
      "Test Episode 7, Total Reward: -2664.0, Steps: 566, Goal Reached: Yes\n",
      "Test Episode 8, Total Reward: -3429.0, Steps: 719, Goal Reached: Yes\n",
      "Test Episode 9, Total Reward: -1464.0, Steps: 326, Goal Reached: Yes\n",
      "Test Episode 10, Total Reward: -847.0, Steps: 199, Goal Reached: Yes\n",
      "Test Episode 11, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Test Episode 12, Total Reward: -3845.0, Steps: 801, Goal Reached: Yes\n",
      "Test Episode 13, Total Reward: -1076.0, Steps: 246, Goal Reached: Yes\n",
      "Test Episode 14, Total Reward: -1203.0, Steps: 275, Goal Reached: Yes\n",
      "Test Episode 15, Total Reward: -526.0, Steps: 136, Goal Reached: Yes\n",
      "Test Episode 16, Total Reward: -381.0, Steps: 107, Goal Reached: Yes\n",
      "Test Episode 17, Total Reward: -3440.0, Steps: 720, Goal Reached: Yes\n",
      "Test Episode 18, Total Reward: -503.0, Steps: 135, Goal Reached: Yes\n",
      "Test Episode 19, Total Reward: -3805.0, Steps: 793, Goal Reached: Yes\n",
      "Test Episode 20, Total Reward: -2750.0, Steps: 559, Goal Reached: No\n",
      "Test Episode 21, Total Reward: -1340.0, Steps: 300, Goal Reached: Yes\n",
      "Test Episode 22, Total Reward: -399.0, Steps: 113, Goal Reached: Yes\n",
      "Test Episode 23, Total Reward: -4304.0, Steps: 894, Goal Reached: Yes\n",
      "Test Episode 24, Total Reward: -116.0, Steps: 54, Goal Reached: Yes\n",
      "Test Episode 25, Total Reward: -201.0, Steps: 71, Goal Reached: Yes\n",
      "Test Episode 26, Total Reward: -2210.0, Steps: 474, Goal Reached: Yes\n",
      "Test Episode 27, Total Reward: -2053.0, Steps: 445, Goal Reached: Yes\n",
      "Test Episode 28, Total Reward: -1350.0, Steps: 302, Goal Reached: Yes\n",
      "Test Episode 29, Total Reward: -1660.0, Steps: 341, Goal Reached: No\n",
      "Test Episode 30, Total Reward: -319.0, Steps: 97, Goal Reached: Yes\n",
      "Test Episode 31, Total Reward: -730.0, Steps: 178, Goal Reached: Yes\n",
      "Test Episode 32, Total Reward: -1561.0, Steps: 343, Goal Reached: Yes\n",
      "Test Episode 33, Total Reward: -879.0, Steps: 209, Goal Reached: Yes\n",
      "Test Episode 34, Total Reward: -86.0, Steps: 48, Goal Reached: Yes\n",
      "Test Episode 35, Total Reward: -8019.0, Steps: 1637, Goal Reached: Yes\n",
      "Test Episode 36, Total Reward: -4160.0, Steps: 864, Goal Reached: Yes\n",
      "Test Episode 37, Total Reward: -3114.0, Steps: 656, Goal Reached: Yes\n",
      "Test Episode 38, Total Reward: -1745.0, Steps: 381, Goal Reached: Yes\n",
      "Test Episode 39, Total Reward: -311.0, Steps: 93, Goal Reached: Yes\n",
      "Test Episode 40, Total Reward: -622.0, Steps: 154, Goal Reached: Yes\n",
      "Test Episode 41, Total Reward: -210.0, Steps: 74, Goal Reached: Yes\n",
      "Test Episode 42, Total Reward: -5650.0, Steps: 1162, Goal Reached: Yes\n",
      "Test Episode 43, Total Reward: -256.0, Steps: 82, Goal Reached: Yes\n",
      "Test Episode 44, Total Reward: -1875.0, Steps: 384, Goal Reached: No\n",
      "Test Episode 45, Total Reward: -1120.0, Steps: 256, Goal Reached: Yes\n",
      "Test Episode 46, Total Reward: -846.0, Steps: 200, Goal Reached: Yes\n",
      "Test Episode 47, Total Reward: -4199.0, Steps: 873, Goal Reached: Yes\n",
      "Test Episode 48, Total Reward: -1290.0, Steps: 290, Goal Reached: Yes\n",
      "Test Episode 49, Total Reward: -3409.0, Steps: 715, Goal Reached: Yes\n",
      "Test Episode 50, Total Reward: -7064.0, Steps: 1446, Goal Reached: Yes\n",
      "Test Episode 51, Total Reward: -689.0, Steps: 171, Goal Reached: Yes\n",
      "Test Episode 52, Total Reward: -1125.0, Steps: 257, Goal Reached: Yes\n",
      "Test Episode 53, Total Reward: -886.0, Steps: 208, Goal Reached: Yes\n",
      "Test Episode 54, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Test Episode 55, Total Reward: -2054.0, Steps: 444, Goal Reached: Yes\n",
      "Test Episode 56, Total Reward: -5994.0, Steps: 1232, Goal Reached: Yes\n",
      "Test Episode 57, Total Reward: -4319.0, Steps: 897, Goal Reached: Yes\n",
      "Test Episode 58, Total Reward: -2059.0, Steps: 422, Goal Reached: No\n",
      "Test Episode 59, Total Reward: -771.0, Steps: 185, Goal Reached: Yes\n",
      "Test Episode 60, Total Reward: -1056.0, Steps: 242, Goal Reached: Yes\n",
      "Test Episode 61, Total Reward: -1974.0, Steps: 428, Goal Reached: Yes\n",
      "Test Episode 62, Total Reward: -1566.0, Steps: 344, Goal Reached: Yes\n",
      "Test Episode 63, Total Reward: -2809.0, Steps: 595, Goal Reached: Yes\n",
      "Test Episode 64, Total Reward: -1055.0, Steps: 243, Goal Reached: Yes\n",
      "Test Episode 65, Total Reward: -2159.0, Steps: 465, Goal Reached: Yes\n",
      "Test Episode 66, Total Reward: -5759.0, Steps: 1185, Goal Reached: Yes\n",
      "Test Episode 67, Total Reward: -455.0, Steps: 123, Goal Reached: Yes\n",
      "Test Episode 68, Total Reward: -4029.0, Steps: 839, Goal Reached: Yes\n",
      "Test Episode 69, Total Reward: -2834.0, Steps: 600, Goal Reached: Yes\n",
      "Test Episode 70, Total Reward: -1261.0, Steps: 283, Goal Reached: Yes\n",
      "Test Episode 71, Total Reward: -879.0, Steps: 209, Goal Reached: Yes\n",
      "Test Episode 72, Total Reward: -567.0, Steps: 143, Goal Reached: Yes\n",
      "Test Episode 73, Total Reward: -1429.0, Steps: 319, Goal Reached: Yes\n",
      "Test Episode 74, Total Reward: -632.0, Steps: 156, Goal Reached: Yes\n",
      "Test Episode 75, Total Reward: -1465.0, Steps: 302, Goal Reached: No\n",
      "Test Episode 76, Total Reward: -2514.0, Steps: 536, Goal Reached: Yes\n",
      "Test Episode 77, Total Reward: -7794.0, Steps: 1592, Goal Reached: Yes\n",
      "Test Episode 78, Total Reward: -3440.0, Steps: 720, Goal Reached: Yes\n",
      "Test Episode 79, Total Reward: -540.0, Steps: 140, Goal Reached: Yes\n",
      "Test Episode 80, Total Reward: -26.0, Steps: 36, Goal Reached: Yes\n",
      "Test Episode 81, Total Reward: -1869.0, Steps: 407, Goal Reached: Yes\n",
      "Test Episode 82, Total Reward: -3120.0, Steps: 633, Goal Reached: No\n",
      "Test Episode 83, Total Reward: -1304.0, Steps: 294, Goal Reached: Yes\n",
      "Test Episode 84, Total Reward: -4941.0, Steps: 1025, Goal Reached: Yes\n",
      "Test Episode 85, Total Reward: -1160.0, Steps: 264, Goal Reached: Yes\n",
      "Test Episode 86, Total Reward: -2214.0, Steps: 476, Goal Reached: Yes\n",
      "Test Episode 87, Total Reward: -1477.0, Steps: 302, Goal Reached: No\n",
      "Test Episode 88, Total Reward: -31.0, Steps: 37, Goal Reached: Yes\n",
      "Test Episode 89, Total Reward: -1110.0, Steps: 254, Goal Reached: Yes\n",
      "Test Episode 90, Total Reward: -412.0, Steps: 112, Goal Reached: Yes\n",
      "Test Episode 91, Total Reward: -576.0, Steps: 146, Goal Reached: Yes\n",
      "Test Episode 92, Total Reward: -1275.0, Steps: 264, Goal Reached: No\n",
      "Test Episode 93, Total Reward: -750.0, Steps: 182, Goal Reached: Yes\n",
      "Test Episode 94, Total Reward: -317.0, Steps: 93, Goal Reached: Yes\n",
      "Test Episode 95, Total Reward: -1425.0, Steps: 294, Goal Reached: No\n",
      "Test Episode 96, Total Reward: -951.0, Steps: 221, Goal Reached: Yes\n",
      "Test Episode 97, Total Reward: -3854.0, Steps: 804, Goal Reached: Yes\n",
      "Test Episode 98, Total Reward: -979.0, Steps: 229, Goal Reached: Yes\n",
      "Test Episode 99, Total Reward: -4000.0, Steps: 832, Goal Reached: Yes\n",
      "Test Episode 100, Total Reward: -2335.0, Steps: 476, Goal Reached: No\n",
      "Test Episode 101, Total Reward: -481.0, Steps: 127, Goal Reached: Yes\n",
      "Test Episode 102, Total Reward: -592.0, Steps: 148, Goal Reached: Yes\n",
      "Test Episode 103, Total Reward: -3953.0, Steps: 825, Goal Reached: Yes\n",
      "Test Episode 104, Total Reward: -680.0, Steps: 168, Goal Reached: Yes\n",
      "Test Episode 105, Total Reward: -901.0, Steps: 188, Goal Reached: No\n",
      "Test Episode 106, Total Reward: -796.0, Steps: 190, Goal Reached: Yes\n",
      "Test Episode 107, Total Reward: -2280.0, Steps: 488, Goal Reached: Yes\n",
      "Test Episode 108, Total Reward: -1665.0, Steps: 365, Goal Reached: Yes\n",
      "Test Episode 109, Total Reward: -102.0, Steps: 50, Goal Reached: Yes\n",
      "Test Episode 110, Total Reward: -157.0, Steps: 61, Goal Reached: Yes\n",
      "Test Episode 111, Total Reward: -2429.0, Steps: 519, Goal Reached: Yes\n",
      "Test Episode 112, Total Reward: -2899.0, Steps: 613, Goal Reached: Yes\n",
      "Test Episode 113, Total Reward: -1564.0, Steps: 346, Goal Reached: Yes\n",
      "Test Episode 114, Total Reward: -2082.0, Steps: 452, Goal Reached: Yes\n",
      "Test Episode 115, Total Reward: -1580.0, Steps: 348, Goal Reached: Yes\n",
      "Test Episode 116, Total Reward: -3493.0, Steps: 733, Goal Reached: Yes\n",
      "Test Episode 117, Total Reward: -3205.0, Steps: 673, Goal Reached: Yes\n",
      "Test Episode 118, Total Reward: -1615.0, Steps: 355, Goal Reached: Yes\n",
      "Test Episode 119, Total Reward: -1776.0, Steps: 386, Goal Reached: Yes\n",
      "Test Episode 120, Total Reward: -3225.0, Steps: 677, Goal Reached: Yes\n",
      "Test Episode 121, Total Reward: -4540.0, Steps: 940, Goal Reached: Yes\n",
      "Test Episode 122, Total Reward: -1977.0, Steps: 425, Goal Reached: Yes\n",
      "Test Episode 123, Total Reward: -1861.0, Steps: 403, Goal Reached: Yes\n",
      "Test Episode 124, Total Reward: -3149.0, Steps: 663, Goal Reached: Yes\n",
      "Test Episode 125, Total Reward: -3744.0, Steps: 782, Goal Reached: Yes\n",
      "Test Episode 126, Total Reward: -2000.0, Steps: 432, Goal Reached: Yes\n",
      "Test Episode 127, Total Reward: -976.0, Steps: 226, Goal Reached: Yes\n",
      "Test Episode 128, Total Reward: -216.0, Steps: 74, Goal Reached: Yes\n",
      "Test Episode 129, Total Reward: -6045.0, Steps: 1218, Goal Reached: No\n",
      "Test Episode 130, Total Reward: -4370.0, Steps: 906, Goal Reached: Yes\n",
      "Test Episode 131, Total Reward: -3200.0, Steps: 672, Goal Reached: Yes\n",
      "Test Episode 132, Total Reward: -1044.0, Steps: 219, Goal Reached: No\n",
      "Test Episode 133, Total Reward: -480.0, Steps: 128, Goal Reached: Yes\n",
      "Test Episode 134, Total Reward: -654.0, Steps: 164, Goal Reached: Yes\n",
      "Test Episode 135, Total Reward: -1118.0, Steps: 258, Goal Reached: Yes\n",
      "Test Episode 136, Total Reward: -1505.0, Steps: 333, Goal Reached: Yes\n",
      "Test Episode 137, Total Reward: -2690.0, Steps: 547, Goal Reached: No\n",
      "Test Episode 138, Total Reward: -1215.0, Steps: 275, Goal Reached: Yes\n",
      "Test Episode 139, Total Reward: -375.0, Steps: 107, Goal Reached: Yes\n",
      "Test Episode 140, Total Reward: -1350.0, Steps: 302, Goal Reached: Yes\n",
      "Test Episode 141, Total Reward: -200.0, Steps: 72, Goal Reached: Yes\n",
      "Test Episode 142, Total Reward: -1241.0, Steps: 279, Goal Reached: Yes\n",
      "Test Episode 143, Total Reward: -611.0, Steps: 153, Goal Reached: Yes\n",
      "Test Episode 144, Total Reward: -7908.0, Steps: 1616, Goal Reached: Yes\n",
      "Test Episode 145, Total Reward: -2920.0, Steps: 616, Goal Reached: Yes\n",
      "Test Episode 146, Total Reward: -1519.0, Steps: 337, Goal Reached: Yes\n",
      "Test Episode 147, Total Reward: -500.0, Steps: 132, Goal Reached: Yes\n",
      "Test Episode 148, Total Reward: -1624.0, Steps: 358, Goal Reached: Yes\n",
      "Test Episode 149, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Test Episode 150, Total Reward: -1526.0, Steps: 342, Goal Reached: Yes\n",
      "Test Episode 151, Total Reward: -1949.0, Steps: 423, Goal Reached: Yes\n",
      "Test Episode 152, Total Reward: -1035.0, Steps: 239, Goal Reached: Yes\n",
      "Test Episode 153, Total Reward: -1113.0, Steps: 257, Goal Reached: Yes\n",
      "Test Episode 154, Total Reward: -2519.0, Steps: 537, Goal Reached: Yes\n",
      "Test Episode 155, Total Reward: -1540.0, Steps: 340, Goal Reached: Yes\n",
      "Test Episode 156, Total Reward: -4463.0, Steps: 927, Goal Reached: Yes\n",
      "Test Episode 157, Total Reward: -3970.0, Steps: 803, Goal Reached: No\n",
      "Test Episode 158, Total Reward: -1690.0, Steps: 370, Goal Reached: Yes\n",
      "Test Episode 159, Total Reward: -2064.0, Steps: 446, Goal Reached: Yes\n",
      "Test Episode 160, Total Reward: -3669.0, Steps: 767, Goal Reached: Yes\n",
      "Test Episode 161, Total Reward: 13.0, Steps: 27, Goal Reached: Yes\n",
      "Test Episode 162, Total Reward: -2653.0, Steps: 565, Goal Reached: Yes\n",
      "Test Episode 163, Total Reward: -1020.0, Steps: 236, Goal Reached: Yes\n",
      "Test Episode 164, Total Reward: -1465.0, Steps: 325, Goal Reached: Yes\n",
      "Test Episode 165, Total Reward: -1290.0, Steps: 290, Goal Reached: Yes\n",
      "Test Episode 166, Total Reward: -2495.0, Steps: 531, Goal Reached: Yes\n",
      "Test Episode 167, Total Reward: -212.0, Steps: 72, Goal Reached: Yes\n",
      "Test Episode 168, Total Reward: -1104.0, Steps: 254, Goal Reached: Yes\n",
      "Test Episode 169, Total Reward: -605.0, Steps: 153, Goal Reached: Yes\n",
      "Test Episode 170, Total Reward: -1045.0, Steps: 241, Goal Reached: Yes\n",
      "Test Episode 171, Total Reward: 68.0, Steps: 16, Goal Reached: Yes\n",
      "Test Episode 172, Total Reward: -1520.0, Steps: 336, Goal Reached: Yes\n",
      "Test Episode 173, Total Reward: -6272.0, Steps: 1290, Goal Reached: Yes\n",
      "Test Episode 174, Total Reward: -1680.0, Steps: 368, Goal Reached: Yes\n",
      "Test Episode 175, Total Reward: -530.0, Steps: 138, Goal Reached: Yes\n",
      "Test Episode 176, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Test Episode 177, Total Reward: -1831.0, Steps: 397, Goal Reached: Yes\n",
      "Test Episode 178, Total Reward: -2532.0, Steps: 542, Goal Reached: Yes\n",
      "Test Episode 179, Total Reward: -1274.0, Steps: 288, Goal Reached: Yes\n",
      "Test Episode 180, Total Reward: -855.0, Steps: 203, Goal Reached: Yes\n",
      "Test Episode 181, Total Reward: -770.0, Steps: 186, Goal Reached: Yes\n",
      "Test Episode 182, Total Reward: -1265.0, Steps: 285, Goal Reached: Yes\n",
      "Test Episode 183, Total Reward: -172.0, Steps: 64, Goal Reached: Yes\n",
      "Test Episode 184, Total Reward: -3480.0, Steps: 728, Goal Reached: Yes\n",
      "Test Episode 185, Total Reward: -1980.0, Steps: 428, Goal Reached: Yes\n",
      "Test Episode 186, Total Reward: -3133.0, Steps: 661, Goal Reached: Yes\n",
      "Test Episode 187, Total Reward: -1130.0, Steps: 258, Goal Reached: Yes\n",
      "Test Episode 188, Total Reward: -20, Steps: 1, Goal Reached: No\n",
      "Test Episode 189, Total Reward: -1884.0, Steps: 410, Goal Reached: Yes\n",
      "Test Episode 190, Total Reward: -3615.0, Steps: 732, Goal Reached: No\n",
      "Test Episode 191, Total Reward: -236.0, Steps: 78, Goal Reached: Yes\n",
      "Test Episode 192, Total Reward: -2286.0, Steps: 488, Goal Reached: Yes\n",
      "Test Episode 193, Total Reward: -1496.0, Steps: 307, Goal Reached: No\n",
      "Test Episode 194, Total Reward: -3489.0, Steps: 731, Goal Reached: Yes\n",
      "Test Episode 195, Total Reward: -697.0, Steps: 169, Goal Reached: Yes\n",
      "Test Episode 196, Total Reward: -2661.0, Steps: 563, Goal Reached: Yes\n",
      "Test Episode 197, Total Reward: -4675.0, Steps: 967, Goal Reached: Yes\n",
      "Test Episode 198, Total Reward: -4524.0, Steps: 938, Goal Reached: Yes\n",
      "Test Episode 199, Total Reward: -1130.0, Steps: 258, Goal Reached: Yes\n",
      "Test Episode 200, Total Reward: -2750.0, Steps: 582, Goal Reached: Yes\n",
      "\n",
      "--- Testing Metrics ---\n",
      "Average Reward: -1924.99\n",
      "Success Rate: 89.00%\n",
      "Average Steps to Goal (for successful episodes): 420.62\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing the trained policy...\")\n",
    "Attention_DQN_metrics = test_attention_dqn(env, policy_net, state_space, action_space, history_size, custom_reward, num_episodes=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
