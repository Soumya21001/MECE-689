{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the FrozenLake environment\n",
    "random.seed(21)\n",
    "np.random.seed(21)\n",
    "torch.manual_seed(21)\n",
    "env = gym.make(\"FrozenLake-v1\",desc=generate_random_map(size=5), is_slippery=False)\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "env.action_space.seed(21)\n",
    "env.observation_space.seed(21)  # Seed the observation space\n",
    "env.reset(seed=21)\n",
    "\n",
    "# Parameters\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.999\n",
    "batch_size = 64\n",
    "buffer_size = 10000\n",
    "target_update_freq = 10\n",
    "num_episodes = 500\n",
    "history_size = 15 # Number of past states to consider\n",
    "backward_steps = 5  # Number of steps for backward planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, history_size):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.history_size = history_size\n",
    "\n",
    "    def add(self, transition):\n",
    "        \"\"\"transition = (state, action, reward, next_state, done).\"\"\"\n",
    "        transition = (transition[0], transition[1], transition[2], transition[3], transition[4])\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)  ##Sample a batch of transitions uniformly\n",
    "        actions, rewards, next_states, dones = zip(*[transition[1:] for transition in batch])\n",
    "\n",
    "        state_histories, reward_histories = [], []\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            state_history, reward_history = self._get_history(idx)\n",
    "            state_histories.append(state_history)\n",
    "            reward_histories.append(reward_history)\n",
    "\n",
    "        return (np.array(state_histories),  # Histories of states\n",
    "            np.array(actions),          # Actions\n",
    "            np.array(rewards),          # Current rewards\n",
    "            np.array(next_states),      # Next states\n",
    "            np.array(dones),            # Done flags\n",
    "            np.array(reward_histories)) # Histories of rewards\n",
    "\n",
    "    def _get_history(self, index):\n",
    "        state_history = []\n",
    "        reward_history = []\n",
    "\n",
    "        for i in range(self.history_size):\n",
    "            if index - i >= 0:\n",
    "                transition = self.buffer[index - i]\n",
    "                state = transition[0]\n",
    "                reward = transition[2]\n",
    "\n",
    "                # Ensuring the state and reward have consistent shapes\n",
    "                state = np.array(state)\n",
    "                if state.ndim == 1:  \n",
    "                    state = state[np.newaxis, :]\n",
    "                state_history.append(state)\n",
    "\n",
    "                reward = np.array(reward)\n",
    "                reward_history.append(reward)\n",
    "            else:\n",
    "                # Padding with zeros if insufficient history\n",
    "                padded_state = np.zeros_like(np.array(self.buffer[0][0]))\n",
    "                if padded_state.ndim == 1:  \n",
    "                    padded_state = padded_state[np.newaxis, :]\n",
    "                \n",
    "                state_history.append(padded_state)\n",
    "                reward_history.append(0)\n",
    "        state_history = np.vstack(state_history)  #  (history_size, state_space)\n",
    "        reward_history = np.array(reward_history).reshape(-1, 1)  # (history_size, 1)\n",
    "\n",
    "        return state_history, reward_history\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size, history_size)  #Calling the replaybuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert state to one-hot encoding\n",
    "def one_hot_state(state, num_states):\n",
    "    one_hot = np.zeros(num_states)\n",
    "    one_hot[state] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_reward(state, action, reward, done, goal_state, visited_states):\n",
    "    if int(state.argmax()) == int(goal_state.argmax()):\n",
    "        return reward + 100  # High reward for reaching the goal\n",
    "    elif done:\n",
    "        return -20  # Heavy penalty for falling\n",
    "    if tuple(state) in visited_states:\n",
    "        return reward-5  # Penalty for revisiting a state (looping)\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mechanism\n",
    "class AttentionMechanism(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(AttentionMechanism, self).__init__()\n",
    "        self.key_layer = nn.Linear(state_dim, hidden_dim)\n",
    "        self.query_layer = nn.Linear(state_dim, hidden_dim)\n",
    "        self.value_layer = nn.Linear(1 , hidden_dim)\n",
    "\n",
    "    def forward(self, query, keys, values):\n",
    "        query = self.query_layer(query).unsqueeze(1)  # (batch_size, 1, hidden_dim)\n",
    "        keys = self.key_layer(keys)                   # (batch_size, seq_len, hidden_dim)\n",
    "        values = self.value_layer(values)             # (batch_size, seq_len, hidden_dim)  (64,10,512)\n",
    "\n",
    "        # Computing attention scores\n",
    "        scores = torch.bmm(query, keys.transpose(1, 2)/ (keys.size(-1) ** 0.5))  # Dot product-  relevance score # (batch_size, 1, seq_len)\n",
    "        \n",
    "\n",
    "        include_rewards = values[..., 0]   # Remove last dim, shape: (batch_size, seq_len)\n",
    "        \n",
    "        include_rewards = include_rewards/ (include_rewards.norm(dim=-1, keepdim=True) + 1e-8)  # Normalize\n",
    "        include_rewards= include_rewards.unsqueeze(1)\n",
    "        scores = scores + include_rewards  # Shape: (batch_size, 1, seq_len)\n",
    "\n",
    "        attention_weights = torch.softmax(scores, dim=-1)  # (batch_size, 1, seq_len)\n",
    "\n",
    "        context = torch.bmm(attention_weights, values).squeeze(1)  # Weighted sum  # (batch_size, hidden_dim)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN with Attention\n",
    "class AttentionDQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(AttentionDQN, self).__init__()\n",
    "        self.attention = AttentionMechanism(state_dim, hidden_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim))\n",
    "\n",
    "    def forward(self, current_state, history, rewards):\n",
    "        \"\"\"\n",
    "        current_state: Current state (batch_size, state_dim)\n",
    "        history: Past states (batch_size, seq_len, state_dim)\n",
    "        rewards: Past rewards (batch_size, seq_len, 1)\n",
    "        \"\"\"\n",
    "        context = self.attention(current_state, history, rewards)  # Applying attention\n",
    "        combined = torch.cat([current_state, context], dim=-1)  # Combine context with current state\n",
    "        return self.fc(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Model\n",
    "class BackwardModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(BackwardModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.state_head = nn.Linear(hidden_dim, state_dim)  # Predicts previous state\n",
    "        self.action_head = nn.Linear(hidden_dim, action_dim)  # Predicts action\n",
    "\n",
    "    def forward(self, current_state):\n",
    "        x = torch.relu(self.fc1(current_state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        previous_state = torch.tanh(self.state_head(x)) \n",
    "        action = torch.softmax(self.action_head(x), dim=-1)\n",
    "        return previous_state, action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Backward Transitions\n",
    "def generate_backward_transitions(goal_state, backward_model, steps):\n",
    "    transitions = []\n",
    "    current_state = goal_state\n",
    "    for _ in range(steps):\n",
    "        with torch.no_grad():\n",
    "            previous_state, action_probs = backward_model(current_state.unsqueeze(0))\n",
    "            action = torch.argmax(action_probs).item()  # Selecting the most probable action\n",
    "            previous_state = previous_state.squeeze(0) \n",
    "\n",
    "        \n",
    "        reward = custom_reward(previous_state, action, 0, done=False, goal_state=goal_state, visited_states=set())\n",
    "        transitions.append((previous_state.numpy(), action, reward, current_state.numpy(), False))\n",
    "        current_state = previous_state\n",
    "    return transitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = AttentionDQN(state_space, action_space)\n",
    "target_net = AttentionDQN(state_space, action_space)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer_forward = optim.Adam(policy_net.parameters(), lr=0.1)\n",
    "backward_model = BackwardModel(state_space, action_space)\n",
    "optimizer_backward = optim.Adam(backward_model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: 15.00, Steps: 36, Goal Reached: No\n",
      "Episode 2, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 3, Total Reward: -6.00, Steps: 15, Goal Reached: No\n",
      "Episode 4, Total Reward: 4.00, Steps: 25, Goal Reached: No\n",
      "Episode 5, Total Reward: -10.00, Steps: 11, Goal Reached: No\n",
      "Episode 6, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 7, Total Reward: -10.00, Steps: 11, Goal Reached: No\n",
      "Episode 8, Total Reward: 10.00, Steps: 31, Goal Reached: No\n",
      "Episode 9, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 10, Total Reward: 122.00, Steps: 22, Goal Reached: Yes\n",
      "Episode 11, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 12, Total Reward: 15.00, Steps: 36, Goal Reached: No\n",
      "Episode 13, Total Reward: -7.00, Steps: 14, Goal Reached: No\n",
      "Episode 14, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 15, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 16, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 17, Total Reward: -4.00, Steps: 17, Goal Reached: No\n",
      "Episode 18, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 19, Total Reward: 0.00, Steps: 21, Goal Reached: No\n",
      "Episode 20, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 21, Total Reward: 21.00, Steps: 42, Goal Reached: No\n",
      "Episode 22, Total Reward: 17.00, Steps: 38, Goal Reached: No\n",
      "Episode 23, Total Reward: 25.00, Steps: 46, Goal Reached: No\n",
      "Episode 24, Total Reward: 10.00, Steps: 31, Goal Reached: No\n",
      "Episode 25, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 26, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 27, Total Reward: 1.00, Steps: 22, Goal Reached: No\n",
      "Episode 28, Total Reward: -1.00, Steps: 20, Goal Reached: No\n",
      "Episode 29, Total Reward: 32.00, Steps: 53, Goal Reached: No\n",
      "Episode 30, Total Reward: -9.00, Steps: 12, Goal Reached: No\n",
      "Episode 31, Total Reward: 9.00, Steps: 30, Goal Reached: No\n",
      "Episode 32, Total Reward: 2.00, Steps: 23, Goal Reached: No\n",
      "Episode 33, Total Reward: -14.00, Steps: 7, Goal Reached: No\n",
      "Episode 34, Total Reward: -8.00, Steps: 13, Goal Reached: No\n",
      "Episode 35, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 36, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 37, Total Reward: 127.00, Steps: 27, Goal Reached: Yes\n",
      "Episode 38, Total Reward: 4.00, Steps: 25, Goal Reached: No\n",
      "Episode 39, Total Reward: 0.00, Steps: 21, Goal Reached: No\n",
      "Episode 40, Total Reward: -5.00, Steps: 16, Goal Reached: No\n",
      "Episode 41, Total Reward: -14.00, Steps: 7, Goal Reached: No\n",
      "Episode 42, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 43, Total Reward: 1.00, Steps: 22, Goal Reached: No\n",
      "Episode 44, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 45, Total Reward: 7.00, Steps: 28, Goal Reached: No\n",
      "Episode 46, Total Reward: -6.00, Steps: 15, Goal Reached: No\n",
      "Episode 47, Total Reward: 4.00, Steps: 25, Goal Reached: No\n",
      "Episode 48, Total Reward: -10.00, Steps: 11, Goal Reached: No\n",
      "Episode 49, Total Reward: -8.00, Steps: 13, Goal Reached: No\n",
      "Episode 50, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 51, Total Reward: 24.00, Steps: 45, Goal Reached: No\n",
      "Episode 52, Total Reward: 8.00, Steps: 29, Goal Reached: No\n",
      "Episode 53, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 54, Total Reward: -10.00, Steps: 11, Goal Reached: No\n",
      "Episode 55, Total Reward: 138.00, Steps: 38, Goal Reached: Yes\n",
      "Episode 56, Total Reward: -4.00, Steps: 17, Goal Reached: No\n",
      "Episode 57, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 58, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 59, Total Reward: -9.00, Steps: 12, Goal Reached: No\n",
      "Episode 60, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 61, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 62, Total Reward: 4.00, Steps: 25, Goal Reached: No\n",
      "Episode 63, Total Reward: 143.00, Steps: 43, Goal Reached: Yes\n",
      "Episode 64, Total Reward: -8.00, Steps: 13, Goal Reached: No\n",
      "Episode 65, Total Reward: -10.00, Steps: 11, Goal Reached: No\n",
      "Episode 66, Total Reward: 2.00, Steps: 23, Goal Reached: No\n",
      "Episode 67, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 68, Total Reward: 14.00, Steps: 35, Goal Reached: No\n",
      "Episode 69, Total Reward: 119.00, Steps: 19, Goal Reached: Yes\n",
      "Episode 70, Total Reward: -14.00, Steps: 7, Goal Reached: No\n",
      "Episode 71, Total Reward: 42.00, Steps: 63, Goal Reached: No\n",
      "Episode 72, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 73, Total Reward: 121.00, Steps: 21, Goal Reached: Yes\n",
      "Episode 74, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 75, Total Reward: -8.00, Steps: 13, Goal Reached: No\n",
      "Episode 76, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 77, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 78, Total Reward: 5.00, Steps: 26, Goal Reached: No\n",
      "Episode 79, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 80, Total Reward: 4.00, Steps: 25, Goal Reached: No\n",
      "Episode 81, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 82, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 83, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 84, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 85, Total Reward: 6.00, Steps: 27, Goal Reached: No\n",
      "Episode 86, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 87, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 88, Total Reward: -10.00, Steps: 11, Goal Reached: No\n",
      "Episode 89, Total Reward: -8.00, Steps: 13, Goal Reached: No\n",
      "Episode 90, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 91, Total Reward: 15.00, Steps: 36, Goal Reached: No\n",
      "Episode 92, Total Reward: 151.00, Steps: 51, Goal Reached: Yes\n",
      "Episode 93, Total Reward: 0.00, Steps: 21, Goal Reached: No\n",
      "Episode 94, Total Reward: -5.00, Steps: 16, Goal Reached: No\n",
      "Episode 95, Total Reward: -9.00, Steps: 12, Goal Reached: No\n",
      "Episode 96, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 97, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 98, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 99, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 100, Total Reward: -8.00, Steps: 13, Goal Reached: No\n",
      "Episode 101, Total Reward: 0.00, Steps: 21, Goal Reached: No\n",
      "Episode 102, Total Reward: 0.00, Steps: 21, Goal Reached: No\n",
      "Episode 103, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 104, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 105, Total Reward: 5.00, Steps: 26, Goal Reached: No\n",
      "Episode 106, Total Reward: -6.00, Steps: 15, Goal Reached: No\n",
      "Episode 107, Total Reward: 142.00, Steps: 42, Goal Reached: Yes\n",
      "Episode 108, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 109, Total Reward: 43.00, Steps: 64, Goal Reached: No\n",
      "Episode 110, Total Reward: -9.00, Steps: 12, Goal Reached: No\n",
      "Episode 111, Total Reward: -4.00, Steps: 17, Goal Reached: No\n",
      "Episode 112, Total Reward: 14.00, Steps: 35, Goal Reached: No\n",
      "Episode 113, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 114, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 115, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 116, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 117, Total Reward: 117.00, Steps: 17, Goal Reached: Yes\n",
      "Episode 118, Total Reward: -8.00, Steps: 13, Goal Reached: No\n",
      "Episode 119, Total Reward: 112.00, Steps: 12, Goal Reached: Yes\n",
      "Episode 120, Total Reward: 1.00, Steps: 22, Goal Reached: No\n",
      "Episode 121, Total Reward: -9.00, Steps: 12, Goal Reached: No\n",
      "Episode 122, Total Reward: 3.00, Steps: 24, Goal Reached: No\n",
      "Episode 123, Total Reward: 1.00, Steps: 22, Goal Reached: No\n",
      "Episode 124, Total Reward: -9.00, Steps: 12, Goal Reached: No\n",
      "Episode 125, Total Reward: -7.00, Steps: 14, Goal Reached: No\n",
      "Episode 126, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 127, Total Reward: -3.00, Steps: 18, Goal Reached: No\n",
      "Episode 128, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 129, Total Reward: 49.00, Steps: 70, Goal Reached: No\n",
      "Episode 130, Total Reward: -1.00, Steps: 20, Goal Reached: No\n",
      "Episode 131, Total Reward: -8.00, Steps: 13, Goal Reached: No\n",
      "Episode 132, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 133, Total Reward: -2.00, Steps: 19, Goal Reached: No\n",
      "Episode 134, Total Reward: -9.00, Steps: 12, Goal Reached: No\n",
      "Episode 135, Total Reward: -14.00, Steps: 7, Goal Reached: No\n",
      "Episode 136, Total Reward: -14.00, Steps: 7, Goal Reached: No\n",
      "Episode 137, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 138, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 139, Total Reward: 153.00, Steps: 53, Goal Reached: Yes\n",
      "Episode 140, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 141, Total Reward: -10.00, Steps: 11, Goal Reached: No\n",
      "Episode 142, Total Reward: 0.00, Steps: 21, Goal Reached: No\n",
      "Episode 143, Total Reward: -8.00, Steps: 13, Goal Reached: No\n",
      "Episode 144, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 145, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 146, Total Reward: -9.00, Steps: 12, Goal Reached: No\n",
      "Episode 147, Total Reward: -9.00, Steps: 12, Goal Reached: No\n",
      "Episode 148, Total Reward: 16.00, Steps: 37, Goal Reached: No\n",
      "Episode 149, Total Reward: 125.00, Steps: 25, Goal Reached: Yes\n",
      "Episode 150, Total Reward: -4.00, Steps: 17, Goal Reached: No\n",
      "Episode 151, Total Reward: 132.00, Steps: 32, Goal Reached: No\n",
      "Episode 152, Total Reward: -7.00, Steps: 14, Goal Reached: No\n",
      "Episode 153, Total Reward: -1.00, Steps: 20, Goal Reached: No\n",
      "Episode 154, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 155, Total Reward: 77.00, Steps: 98, Goal Reached: No\n",
      "Episode 156, Total Reward: 8.00, Steps: 29, Goal Reached: No\n",
      "Episode 157, Total Reward: -4.00, Steps: 17, Goal Reached: No\n",
      "Episode 158, Total Reward: -2.00, Steps: 19, Goal Reached: No\n",
      "Episode 159, Total Reward: -14.00, Steps: 7, Goal Reached: No\n",
      "Episode 160, Total Reward: 21.00, Steps: 42, Goal Reached: No\n",
      "Episode 161, Total Reward: -14.00, Steps: 7, Goal Reached: No\n",
      "Episode 162, Total Reward: 129.00, Steps: 29, Goal Reached: Yes\n",
      "Episode 163, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 164, Total Reward: -14.00, Steps: 7, Goal Reached: No\n",
      "Episode 165, Total Reward: -3.00, Steps: 18, Goal Reached: No\n",
      "Episode 166, Total Reward: -6.00, Steps: 15, Goal Reached: No\n",
      "Episode 167, Total Reward: 109.00, Steps: 9, Goal Reached: Yes\n",
      "Episode 168, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 169, Total Reward: 5.00, Steps: 26, Goal Reached: No\n",
      "Episode 170, Total Reward: -7.00, Steps: 14, Goal Reached: No\n",
      "Episode 171, Total Reward: 27.00, Steps: 48, Goal Reached: No\n",
      "Episode 172, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 173, Total Reward: 5.00, Steps: 26, Goal Reached: No\n",
      "Episode 174, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 175, Total Reward: 14.00, Steps: 35, Goal Reached: No\n",
      "Episode 176, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 177, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 178, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 179, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 180, Total Reward: -10.00, Steps: 11, Goal Reached: No\n",
      "Episode 181, Total Reward: 17.00, Steps: 38, Goal Reached: No\n",
      "Episode 182, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 183, Total Reward: 1.00, Steps: 22, Goal Reached: No\n",
      "Episode 184, Total Reward: -10.00, Steps: 11, Goal Reached: No\n",
      "Episode 185, Total Reward: 10.00, Steps: 31, Goal Reached: No\n",
      "Episode 186, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 187, Total Reward: 24.00, Steps: 45, Goal Reached: No\n",
      "Episode 188, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 189, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 190, Total Reward: 4.00, Steps: 25, Goal Reached: No\n",
      "Episode 191, Total Reward: -7.00, Steps: 14, Goal Reached: No\n",
      "Episode 192, Total Reward: 140.00, Steps: 40, Goal Reached: Yes\n",
      "Episode 193, Total Reward: 112.00, Steps: 12, Goal Reached: Yes\n",
      "Episode 194, Total Reward: 21.00, Steps: 42, Goal Reached: No\n",
      "Episode 195, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 196, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 197, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 198, Total Reward: -4.00, Steps: 17, Goal Reached: No\n",
      "Episode 199, Total Reward: 132.00, Steps: 32, Goal Reached: Yes\n",
      "Episode 200, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 201, Total Reward: -14.00, Steps: 7, Goal Reached: No\n",
      "Episode 202, Total Reward: 9.00, Steps: 30, Goal Reached: No\n",
      "Episode 203, Total Reward: 113.00, Steps: 13, Goal Reached: Yes\n",
      "Episode 204, Total Reward: -7.00, Steps: 14, Goal Reached: No\n",
      "Episode 205, Total Reward: 21.00, Steps: 42, Goal Reached: No\n",
      "Episode 206, Total Reward: 113.00, Steps: 13, Goal Reached: Yes\n",
      "Episode 207, Total Reward: 115.00, Steps: 15, Goal Reached: Yes\n",
      "Episode 208, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 209, Total Reward: 4.00, Steps: 25, Goal Reached: No\n",
      "Episode 210, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 211, Total Reward: 158.00, Steps: 58, Goal Reached: No\n",
      "Episode 212, Total Reward: -4.00, Steps: 17, Goal Reached: No\n",
      "Episode 213, Total Reward: -3.00, Steps: 18, Goal Reached: No\n",
      "Episode 214, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 215, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 216, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 217, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 218, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 219, Total Reward: -7.00, Steps: 14, Goal Reached: No\n",
      "Episode 220, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 221, Total Reward: 13.00, Steps: 34, Goal Reached: No\n",
      "Episode 222, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 223, Total Reward: -14.00, Steps: 7, Goal Reached: No\n",
      "Episode 224, Total Reward: 3.00, Steps: 24, Goal Reached: No\n",
      "Episode 225, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 226, Total Reward: -3.00, Steps: 18, Goal Reached: No\n",
      "Episode 227, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 228, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 229, Total Reward: 5.00, Steps: 26, Goal Reached: No\n",
      "Episode 230, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 231, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 232, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 233, Total Reward: 25.00, Steps: 46, Goal Reached: No\n",
      "Episode 234, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 235, Total Reward: -5.00, Steps: 16, Goal Reached: No\n",
      "Episode 236, Total Reward: 37.00, Steps: 58, Goal Reached: No\n",
      "Episode 237, Total Reward: 18.00, Steps: 39, Goal Reached: No\n",
      "Episode 238, Total Reward: -7.00, Steps: 14, Goal Reached: No\n",
      "Episode 239, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 240, Total Reward: 119.00, Steps: 19, Goal Reached: Yes\n",
      "Episode 241, Total Reward: 137.00, Steps: 37, Goal Reached: No\n",
      "Episode 242, Total Reward: 3.00, Steps: 24, Goal Reached: No\n",
      "Episode 243, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 244, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 245, Total Reward: 112.00, Steps: 12, Goal Reached: Yes\n",
      "Episode 246, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 247, Total Reward: 4.00, Steps: 25, Goal Reached: No\n",
      "Episode 248, Total Reward: 13.00, Steps: 34, Goal Reached: No\n",
      "Episode 249, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 250, Total Reward: 6.00, Steps: 27, Goal Reached: No\n",
      "Episode 251, Total Reward: -14.00, Steps: 7, Goal Reached: No\n",
      "Episode 252, Total Reward: 5.00, Steps: 26, Goal Reached: No\n",
      "Episode 253, Total Reward: 30.00, Steps: 51, Goal Reached: No\n",
      "Episode 254, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 255, Total Reward: 0.00, Steps: 21, Goal Reached: No\n",
      "Episode 256, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 257, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 258, Total Reward: 19.00, Steps: 40, Goal Reached: No\n",
      "Episode 259, Total Reward: -3.00, Steps: 18, Goal Reached: No\n",
      "Episode 260, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 261, Total Reward: 128.00, Steps: 28, Goal Reached: No\n",
      "Episode 262, Total Reward: 135.00, Steps: 35, Goal Reached: Yes\n",
      "Episode 263, Total Reward: -9.00, Steps: 12, Goal Reached: No\n",
      "Episode 264, Total Reward: -10.00, Steps: 11, Goal Reached: No\n",
      "Episode 265, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 266, Total Reward: -14.00, Steps: 7, Goal Reached: No\n",
      "Episode 267, Total Reward: -9.00, Steps: 12, Goal Reached: No\n",
      "Episode 268, Total Reward: -2.00, Steps: 19, Goal Reached: No\n",
      "Episode 269, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 270, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 271, Total Reward: -7.00, Steps: 14, Goal Reached: No\n",
      "Episode 272, Total Reward: 125.00, Steps: 25, Goal Reached: Yes\n",
      "Episode 273, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 274, Total Reward: -6.00, Steps: 15, Goal Reached: No\n",
      "Episode 275, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 276, Total Reward: 171.00, Steps: 71, Goal Reached: Yes\n",
      "Episode 277, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 278, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 279, Total Reward: -8.00, Steps: 13, Goal Reached: No\n",
      "Episode 280, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 281, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 282, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 283, Total Reward: 156.00, Steps: 56, Goal Reached: Yes\n",
      "Episode 284, Total Reward: 129.00, Steps: 29, Goal Reached: Yes\n",
      "Episode 285, Total Reward: 0.00, Steps: 21, Goal Reached: No\n",
      "Episode 286, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 287, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 288, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 289, Total Reward: -10.00, Steps: 11, Goal Reached: No\n",
      "Episode 290, Total Reward: 139.00, Steps: 39, Goal Reached: Yes\n",
      "Episode 291, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 292, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 293, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 294, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 295, Total Reward: -7.00, Steps: 14, Goal Reached: No\n",
      "Episode 296, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 297, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 298, Total Reward: 160.00, Steps: 60, Goal Reached: Yes\n",
      "Episode 299, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 300, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 301, Total Reward: 27.00, Steps: 48, Goal Reached: No\n",
      "Episode 302, Total Reward: 62.00, Steps: 83, Goal Reached: No\n",
      "Episode 303, Total Reward: 1.00, Steps: 22, Goal Reached: No\n",
      "Episode 304, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 305, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 306, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 307, Total Reward: -10.00, Steps: 11, Goal Reached: No\n",
      "Episode 308, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 309, Total Reward: 136.00, Steps: 36, Goal Reached: Yes\n",
      "Episode 310, Total Reward: 184.00, Steps: 84, Goal Reached: Yes\n",
      "Episode 311, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 312, Total Reward: -9.00, Steps: 12, Goal Reached: No\n",
      "Episode 313, Total Reward: -10.00, Steps: 11, Goal Reached: No\n",
      "Episode 314, Total Reward: -9.00, Steps: 12, Goal Reached: No\n",
      "Episode 315, Total Reward: 5.00, Steps: 26, Goal Reached: No\n",
      "Episode 316, Total Reward: 5.00, Steps: 26, Goal Reached: No\n",
      "Episode 317, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 318, Total Reward: 117.00, Steps: 17, Goal Reached: Yes\n",
      "Episode 319, Total Reward: -6.00, Steps: 15, Goal Reached: No\n",
      "Episode 320, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 321, Total Reward: 1.00, Steps: 22, Goal Reached: No\n",
      "Episode 322, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 323, Total Reward: 135.00, Steps: 35, Goal Reached: Yes\n",
      "Episode 324, Total Reward: 131.00, Steps: 31, Goal Reached: Yes\n",
      "Episode 325, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 326, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 327, Total Reward: 45.00, Steps: 66, Goal Reached: No\n",
      "Episode 328, Total Reward: -14.00, Steps: 7, Goal Reached: No\n",
      "Episode 329, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 330, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 331, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 332, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 333, Total Reward: -4.00, Steps: 17, Goal Reached: No\n",
      "Episode 334, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 335, Total Reward: 29.00, Steps: 50, Goal Reached: No\n",
      "Episode 336, Total Reward: 171.00, Steps: 71, Goal Reached: Yes\n",
      "Episode 337, Total Reward: 4.00, Steps: 25, Goal Reached: No\n",
      "Episode 338, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 339, Total Reward: -8.00, Steps: 13, Goal Reached: No\n",
      "Episode 340, Total Reward: 145.00, Steps: 45, Goal Reached: Yes\n",
      "Episode 341, Total Reward: 8.00, Steps: 29, Goal Reached: No\n",
      "Episode 342, Total Reward: -5.00, Steps: 16, Goal Reached: No\n",
      "Episode 343, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 344, Total Reward: 51.00, Steps: 72, Goal Reached: No\n",
      "Episode 345, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 346, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 347, Total Reward: 7.00, Steps: 28, Goal Reached: No\n",
      "Episode 348, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 349, Total Reward: 12.00, Steps: 33, Goal Reached: No\n",
      "Episode 350, Total Reward: 3.00, Steps: 24, Goal Reached: No\n",
      "Episode 351, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 352, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 353, Total Reward: -14.00, Steps: 7, Goal Reached: No\n",
      "Episode 354, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 355, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 356, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 357, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 358, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 359, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 360, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 361, Total Reward: -9.00, Steps: 12, Goal Reached: No\n",
      "Episode 362, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 363, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 364, Total Reward: 9.00, Steps: 30, Goal Reached: No\n",
      "Episode 365, Total Reward: 13.00, Steps: 34, Goal Reached: No\n",
      "Episode 366, Total Reward: 5.00, Steps: 26, Goal Reached: No\n",
      "Episode 367, Total Reward: -7.00, Steps: 14, Goal Reached: No\n",
      "Episode 368, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 369, Total Reward: -3.00, Steps: 18, Goal Reached: No\n",
      "Episode 370, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 371, Total Reward: 116.00, Steps: 16, Goal Reached: No\n",
      "Episode 372, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 373, Total Reward: -8.00, Steps: 13, Goal Reached: No\n",
      "Episode 374, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 375, Total Reward: 54.00, Steps: 75, Goal Reached: No\n",
      "Episode 376, Total Reward: -5.00, Steps: 16, Goal Reached: No\n",
      "Episode 377, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 378, Total Reward: 9.00, Steps: 30, Goal Reached: No\n",
      "Episode 379, Total Reward: -8.00, Steps: 13, Goal Reached: No\n",
      "Episode 380, Total Reward: 145.00, Steps: 45, Goal Reached: Yes\n",
      "Episode 381, Total Reward: 5.00, Steps: 26, Goal Reached: No\n",
      "Episode 382, Total Reward: 17.00, Steps: 38, Goal Reached: No\n",
      "Episode 383, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 384, Total Reward: 126.00, Steps: 26, Goal Reached: Yes\n",
      "Episode 385, Total Reward: 6.00, Steps: 27, Goal Reached: No\n",
      "Episode 386, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 387, Total Reward: -10.00, Steps: 11, Goal Reached: No\n",
      "Episode 388, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 389, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 390, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 391, Total Reward: 42.00, Steps: 63, Goal Reached: No\n",
      "Episode 392, Total Reward: -5.00, Steps: 16, Goal Reached: No\n",
      "Episode 393, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 394, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 395, Total Reward: 166.00, Steps: 66, Goal Reached: Yes\n",
      "Episode 396, Total Reward: -14.00, Steps: 7, Goal Reached: No\n",
      "Episode 397, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 398, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 399, Total Reward: 16.00, Steps: 37, Goal Reached: No\n",
      "Episode 400, Total Reward: -9.00, Steps: 12, Goal Reached: No\n",
      "Episode 401, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 402, Total Reward: 12.00, Steps: 33, Goal Reached: No\n",
      "Episode 403, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 404, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 405, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 406, Total Reward: -2.00, Steps: 19, Goal Reached: No\n",
      "Episode 407, Total Reward: -5.00, Steps: 16, Goal Reached: No\n",
      "Episode 408, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 409, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 410, Total Reward: 28.00, Steps: 49, Goal Reached: No\n",
      "Episode 411, Total Reward: -7.00, Steps: 14, Goal Reached: No\n",
      "Episode 412, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 413, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 414, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 415, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 416, Total Reward: 181.00, Steps: 81, Goal Reached: Yes\n",
      "Episode 417, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 418, Total Reward: 0.00, Steps: 21, Goal Reached: No\n",
      "Episode 419, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 420, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 421, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 422, Total Reward: -1.00, Steps: 20, Goal Reached: No\n",
      "Episode 423, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 424, Total Reward: 123.00, Steps: 23, Goal Reached: Yes\n",
      "Episode 425, Total Reward: -14.00, Steps: 7, Goal Reached: No\n",
      "Episode 426, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 427, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 428, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 429, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 430, Total Reward: -10.00, Steps: 11, Goal Reached: No\n",
      "Episode 431, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 432, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 433, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 434, Total Reward: 17.00, Steps: 38, Goal Reached: No\n",
      "Episode 435, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 436, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 437, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 438, Total Reward: 130.00, Steps: 30, Goal Reached: Yes\n",
      "Episode 439, Total Reward: 134.00, Steps: 34, Goal Reached: Yes\n",
      "Episode 440, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 441, Total Reward: 6.00, Steps: 27, Goal Reached: No\n",
      "Episode 442, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 443, Total Reward: 130.00, Steps: 30, Goal Reached: Yes\n",
      "Episode 444, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 445, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 446, Total Reward: 23.00, Steps: 44, Goal Reached: No\n",
      "Episode 447, Total Reward: 151.00, Steps: 51, Goal Reached: Yes\n",
      "Episode 448, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 449, Total Reward: 27.00, Steps: 48, Goal Reached: No\n",
      "Episode 450, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 451, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 452, Total Reward: 25.00, Steps: 46, Goal Reached: No\n",
      "Episode 453, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 454, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 455, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 456, Total Reward: 24.00, Steps: 45, Goal Reached: No\n",
      "Episode 457, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 458, Total Reward: 22.00, Steps: 43, Goal Reached: No\n",
      "Episode 459, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 460, Total Reward: -12.00, Steps: 9, Goal Reached: No\n",
      "Episode 461, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 462, Total Reward: 33.00, Steps: 54, Goal Reached: No\n",
      "Episode 463, Total Reward: -6.00, Steps: 15, Goal Reached: No\n",
      "Episode 464, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 465, Total Reward: -7.00, Steps: 14, Goal Reached: No\n",
      "Episode 466, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 467, Total Reward: 112.00, Steps: 12, Goal Reached: Yes\n",
      "Episode 468, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 469, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 470, Total Reward: -14.00, Steps: 7, Goal Reached: No\n",
      "Episode 471, Total Reward: 44.00, Steps: 65, Goal Reached: No\n",
      "Episode 472, Total Reward: -11.00, Steps: 10, Goal Reached: No\n",
      "Episode 473, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 474, Total Reward: 23.00, Steps: 44, Goal Reached: No\n",
      "Episode 475, Total Reward: 149.00, Steps: 49, Goal Reached: Yes\n",
      "Episode 476, Total Reward: 145.00, Steps: 45, Goal Reached: Yes\n",
      "Episode 477, Total Reward: 128.00, Steps: 28, Goal Reached: Yes\n",
      "Episode 478, Total Reward: 76.00, Steps: 97, Goal Reached: No\n",
      "Episode 479, Total Reward: 11.00, Steps: 32, Goal Reached: No\n",
      "Episode 480, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 481, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 482, Total Reward: 218.00, Steps: 118, Goal Reached: Yes\n",
      "Episode 483, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 484, Total Reward: 118.00, Steps: 18, Goal Reached: Yes\n",
      "Episode 485, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 486, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 487, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 488, Total Reward: -13.00, Steps: 8, Goal Reached: No\n",
      "Episode 489, Total Reward: 136.00, Steps: 36, Goal Reached: Yes\n",
      "Episode 490, Total Reward: -15.00, Steps: 6, Goal Reached: No\n",
      "Episode 491, Total Reward: 20.00, Steps: 41, Goal Reached: No\n",
      "Episode 492, Total Reward: -8.00, Steps: 13, Goal Reached: No\n",
      "Episode 493, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 494, Total Reward: -18.00, Steps: 3, Goal Reached: No\n",
      "Episode 495, Total Reward: 0.00, Steps: 21, Goal Reached: No\n",
      "Episode 496, Total Reward: -17.00, Steps: 4, Goal Reached: No\n",
      "Episode 497, Total Reward: -16.00, Steps: 5, Goal Reached: No\n",
      "Episode 498, Total Reward: 41.00, Steps: 62, Goal Reached: No\n",
      "Episode 499, Total Reward: 119.00, Steps: 19, Goal Reached: Yes\n",
      "Episode 500, Total Reward: 129.00, Steps: 29, Goal Reached: Yes\n",
      "\n",
      "--- Training Metrics ---\n",
      "Average Reward: 11.26\n",
      "Success Rate: 10.80%\n",
      "Average Steps to Goal (for successful episodes): 35.89\n",
      "------------------------\n",
      "Training complete. Model saved.\n",
      "Backward model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "#Training Loop\n",
    "total_rewards = []\n",
    "success_count = 0\n",
    "steps_to_goal = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = one_hot_state(state, state_space)\n",
    "    goal_state = one_hot_state(state_space - 1, state_space) # Assumption - the last state corresponds to the goal\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    visited_states = set()\n",
    "    history = deque(maxlen=history_size)\n",
    "    rewards = deque(maxlen=history_size)\n",
    "\n",
    "    while not done:\n",
    "        while len(history) < history_size:\n",
    "            history.append(np.zeros_like(state)) # Padding the history with zeros before enough steps are taken.\n",
    "            rewards.append(0)\n",
    "\n",
    "        # Converting histories to tensors\n",
    "        history_tensor = torch.tensor(np.array(history), dtype=torch.float32).unsqueeze(0)\n",
    "        rewards_tensor = torch.tensor(np.array(rewards), dtype=torch.float32).unsqueeze(-1).unsqueeze(0)\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(range(action_space))  # Exploration\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = torch.argmax(policy_net(state_tensor, history_tensor, rewards_tensor)).item()  # Exploitation\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = one_hot_state(next_state, state_space)\n",
    "\n",
    "        reward = custom_reward(next_state, action, reward, done, goal_state, visited_states=set())\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "\n",
    "        visited_states.add(tuple(next_state))  #updating visiting states for checking looping\n",
    "\n",
    "        # Update histories\n",
    "        history.append(state)\n",
    "        rewards.append(reward)\n",
    "        replay_buffer.add((state, action, reward, next_state, done))  # Storing transition in replay buffer\n",
    "        state = next_state\n",
    "\n",
    "        # Training the Q-Network\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            batch = replay_buffer.sample(batch_size)\n",
    "            state_histories, actions, rewards_batch, next_states, dones, reward_histories = batch\n",
    "\n",
    "            state_histories = torch.tensor(state_histories, dtype=torch.float32)\n",
    "            reward_histories = torch.tensor(reward_histories, dtype=torch.float32)#.unsqueeze(-1)\n",
    "            actions = torch.tensor(actions, dtype=torch.long)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "            # Computing target Q-values\n",
    "            with torch.no_grad():\n",
    "                next_state_histories = torch.cat((state_histories[:, 1:, :], next_states.unsqueeze(1)), dim=1)\n",
    "                next_reward_histories = torch.cat((reward_histories[:, 1:, :], rewards_batch.unsqueeze(1).unsqueeze(-1)), dim=1)\n",
    "                target_q_values = rewards_batch + gamma * (1 - dones) * torch.max(\n",
    "                    target_net(next_states, next_state_histories, next_reward_histories), dim=1)[0]\n",
    "\n",
    "            current_q_values = policy_net(state_histories[:, -1, :], state_histories, reward_histories).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "            optimizer_forward.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer_forward.step()\n",
    "\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "    if episode % target_update_freq == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    # Adding backward transitions every 10 episodes\n",
    "    if episode % 10 == 0:\n",
    "        backward_transitions = generate_backward_transitions(torch.tensor(goal_state, dtype=torch.float32), backward_model, backward_steps)\n",
    "        for transition in backward_transitions:\n",
    "            replay_buffer.add(transition)\n",
    "        \n",
    "        # Training the backward model\n",
    "        for transition in backward_transitions:\n",
    "            state, action, reward, next_state, done = transition\n",
    "\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "            action_tensor = torch.tensor([action], dtype=torch.long)\n",
    "\n",
    "            # Predicted backward state-action pairs\n",
    "            predicted_state, predicted_action = backward_model(next_state_tensor)\n",
    "\n",
    "            state_loss = nn.MSELoss()(predicted_state, state_tensor)\n",
    "            action_loss = nn.CrossEntropyLoss()(predicted_action, action_tensor)\n",
    "            backward_loss = state_loss + action_loss\n",
    "\n",
    "            optimizer_backward.zero_grad()\n",
    "            backward_loss.backward()\n",
    "            optimizer_backward.step()\n",
    "\n",
    "    total_rewards.append(total_reward)\n",
    "    if done and int(state.argmax()) == int(goal_state.argmax()):\n",
    "        success_count += 1\n",
    "        steps_to_goal.append(step_count)\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Total Reward: {total_reward:.2f}, Steps: {step_count}, Goal Reached: {'Yes' if done and int(state.argmax()) == int(goal_state.argmax()) else 'No'}\")\n",
    "\n",
    "# Calculating metrics\n",
    "avg_reward = np.mean(total_rewards)\n",
    "success_rate = (success_count / num_episodes) * 100\n",
    "avg_steps = np.mean(steps_to_goal) if steps_to_goal else float('inf')\n",
    "\n",
    "print(\"\\n--- Training Metrics ---\")\n",
    "print(f\"Average Reward: {avg_reward:.2f}\")\n",
    "print(f\"Success Rate: {success_rate:.2f}%\")\n",
    "print(f\"Average Steps to Goal (for successful episodes): {avg_steps:.2f}\")\n",
    "print(\"------------------------\")\n",
    "\n",
    "# Saving the trained model\n",
    "torch.save(policy_net.state_dict(), \"model/Backward_DQN_policy_net.pth\")\n",
    "print(\"Training complete. Model saved.\")\n",
    "\n",
    "# Saving the backward model\n",
    "torch.save(backward_model.state_dict(), \"model/Backward_DQN_backward_model.pth\")\n",
    "print(\"Backward model saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop \n",
    "def test_model_with_backward(policy_net, backward_model, env, num_test_episodes=100, history_size=10, use_backward=False):\n",
    "    total_rewards = []\n",
    "    success_count = 0\n",
    "    steps_to_goal = []\n",
    "\n",
    "    policy_net.load_state_dict(torch.load(\"model/Backward_DQN_policy_net.pth\"))\n",
    "    policy_net.eval()  \n",
    "    backward_model.load_state_dict(torch.load(\"model/DQN_trained_backward_model.pth\"))\n",
    "    backward_model.eval()  \n",
    "\n",
    "    for episode in range(num_test_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = one_hot_state(state, env.observation_space.n)\n",
    "        goal_state = one_hot_state(env.observation_space.n - 1, env.observation_space.n)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "        visited_states = set()\n",
    "        history = deque(maxlen=history_size)\n",
    "        rewards = deque(maxlen=history_size)\n",
    "\n",
    "        while not done:\n",
    "            while len(history) < history_size:\n",
    "                history.append(np.zeros_like(state))\n",
    "                rewards.append(0)\n",
    "\n",
    "            history_tensor = torch.tensor(np.array(history), dtype=torch.float32).unsqueeze(0)\n",
    "            rewards_tensor = torch.tensor(np.array(rewards), dtype=torch.float32).unsqueeze(-1).unsqueeze(0)\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            if random.random() < 0.1:  # 10% chance to explore\n",
    "                     action = random.choice(range(action_space))\n",
    "            else: \n",
    "                with torch.no_grad():\n",
    "                    action = torch.argmax(policy_net(state_tensor, history_tensor, rewards_tensor)).item()\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = one_hot_state(next_state, env.observation_space.n)\n",
    "\n",
    "            reward = custom_reward(next_state, action, reward, done, goal_state, visited_states)\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "            visited_states.add(tuple(next_state))\n",
    "            history.append(state)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            # If backward planning is enabled\n",
    "            if use_backward and step_count % 10 == 0:\n",
    "                with torch.no_grad():\n",
    "                    backward_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "                    predicted_state, predicted_action = backward_model(backward_state_tensor)\n",
    "\n",
    "\n",
    "        # Updating metrics\n",
    "        total_rewards.append(total_reward)\n",
    "        if int(state.argmax()) == int(goal_state.argmax()):\n",
    "            success_count += 1\n",
    "            steps_to_goal.append(step_count)\n",
    "\n",
    "        print(f\"Test Episode {episode + 1}: Reward = {total_reward:.2f}, Steps = {step_count}, Goal Reached: {'Yes' if done and int(state.argmax()) == int(goal_state.argmax()) else 'No'}\")\n",
    "\n",
    "    # Calculate final metrics\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    success_rate = (success_count / num_test_episodes) * 100\n",
    "    avg_steps = np.mean(steps_to_goal) if steps_to_goal else float('inf')\n",
    "\n",
    "    print(\"\\n--- Testing Metrics ---\")\n",
    "    print(f\"Average Reward: {avg_reward:.2f}\")\n",
    "    print(f\"Success Rate: {success_rate:.2f}%\")\n",
    "    print(f\"Average Steps to Goal (for successful episodes): {avg_steps:.2f}\")\n",
    "    print(\"------------------------\")\n",
    "\n",
    "# Return metrics as a dictionary\n",
    "    metrics = {\"Average Reward\": avg_reward,\n",
    "        \"Success Rate (%)\": success_rate,\n",
    "        \"Average Steps to Goal\": avg_steps}\n",
    "    return metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soumya Taneja\\AppData\\Local\\Temp\\ipykernel_4332\\3856074015.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_net.load_state_dict(torch.load(\"Backward_DQN_policy_net.pth\"))\n",
      "C:\\Users\\Soumya Taneja\\AppData\\Local\\Temp\\ipykernel_4332\\3856074015.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  backward_model.load_state_dict(torch.load(\"DQN_trained_backward_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 1: Reward = -16.00, Steps = 5, Goal Reached: No\n",
      "Test Episode 2: Reward = -2944.00, Steps = 622, Goal Reached: Yes\n",
      "Test Episode 3: Reward = -56.00, Steps = 19, Goal Reached: No\n",
      "Test Episode 4: Reward = -17.00, Steps = 33, Goal Reached: Yes\n",
      "Test Episode 5: Reward = -908.00, Steps = 216, Goal Reached: Yes\n",
      "Test Episode 6: Reward = -738.00, Steps = 153, Goal Reached: No\n",
      "Test Episode 7: Reward = -610.00, Steps = 154, Goal Reached: Yes\n",
      "Test Episode 8: Reward = -17.00, Steps = 4, Goal Reached: No\n",
      "Test Episode 9: Reward = -1203.00, Steps = 275, Goal Reached: Yes\n",
      "Test Episode 10: Reward = -62.00, Steps = 42, Goal Reached: Yes\n",
      "Test Episode 11: Reward = -436.00, Steps = 118, Goal Reached: Yes\n",
      "Test Episode 12: Reward = -619.00, Steps = 157, Goal Reached: Yes\n",
      "Test Episode 13: Reward = -105.00, Steps = 24, Goal Reached: No\n",
      "Test Episode 14: Reward = -879.00, Steps = 186, Goal Reached: No\n",
      "Test Episode 15: Reward = -535.00, Steps = 139, Goal Reached: Yes\n",
      "Test Episode 16: Reward = -913.00, Steps = 194, Goal Reached: No\n",
      "Test Episode 17: Reward = -869.00, Steps = 207, Goal Reached: Yes\n",
      "Test Episode 18: Reward = -100.00, Steps = 52, Goal Reached: Yes\n",
      "Test Episode 19: Reward = -30.00, Steps = 38, Goal Reached: Yes\n",
      "Test Episode 20: Reward = -187.00, Steps = 67, Goal Reached: Yes\n",
      "Test Episode 21: Reward = -763.00, Steps = 187, Goal Reached: Yes\n",
      "Test Episode 22: Reward = -201.00, Steps = 71, Goal Reached: Yes\n",
      "Test Episode 23: Reward = -18.00, Steps = 3, Goal Reached: No\n",
      "Test Episode 24: Reward = -791.00, Steps = 189, Goal Reached: Yes\n",
      "Test Episode 25: Reward = -171.00, Steps = 65, Goal Reached: Yes\n",
      "Test Episode 26: Reward = 13.00, Steps = 27, Goal Reached: Yes\n",
      "Test Episode 27: Reward = -1170.00, Steps = 266, Goal Reached: Yes\n",
      "Test Episode 28: Reward = -17.00, Steps = 4, Goal Reached: No\n",
      "Test Episode 29: Reward = -156.00, Steps = 62, Goal Reached: Yes\n",
      "Test Episode 30: Reward = -778.00, Steps = 190, Goal Reached: Yes\n",
      "Test Episode 31: Reward = -580.00, Steps = 148, Goal Reached: Yes\n",
      "Test Episode 32: Reward = -86.00, Steps = 48, Goal Reached: Yes\n",
      "Test Episode 33: Reward = -216.00, Steps = 74, Goal Reached: Yes\n",
      "Test Episode 34: Reward = -1675.00, Steps = 344, Goal Reached: No\n",
      "Test Episode 35: Reward = -1239.00, Steps = 281, Goal Reached: Yes\n",
      "Test Episode 36: Reward = -960.00, Steps = 224, Goal Reached: Yes\n",
      "Test Episode 37: Reward = -1575.00, Steps = 324, Goal Reached: No\n",
      "Test Episode 38: Reward = -615.00, Steps = 155, Goal Reached: Yes\n",
      "Test Episode 39: Reward = -509.00, Steps = 135, Goal Reached: Yes\n",
      "Test Episode 40: Reward = -1123.00, Steps = 259, Goal Reached: Yes\n",
      "Test Episode 41: Reward = -924.00, Steps = 218, Goal Reached: Yes\n",
      "Test Episode 42: Reward = -22.00, Steps = 5, Goal Reached: No\n",
      "Test Episode 43: Reward = -164.00, Steps = 37, Goal Reached: No\n",
      "Test Episode 44: Reward = -1054.00, Steps = 244, Goal Reached: Yes\n",
      "Test Episode 45: Reward = -798.00, Steps = 194, Goal Reached: Yes\n",
      "Test Episode 46: Reward = -49.00, Steps = 14, Goal Reached: No\n",
      "Test Episode 47: Reward = -199.00, Steps = 44, Goal Reached: No\n",
      "Test Episode 48: Reward = -2869.00, Steps = 584, Goal Reached: No\n",
      "Test Episode 49: Reward = -18.00, Steps = 3, Goal Reached: No\n",
      "Test Episode 50: Reward = -609.00, Steps = 155, Goal Reached: Yes\n",
      "Test Episode 51: Reward = -238.00, Steps = 53, Goal Reached: No\n",
      "Test Episode 52: Reward = -199.00, Steps = 44, Goal Reached: No\n",
      "Test Episode 53: Reward = -1044.00, Steps = 242, Goal Reached: Yes\n",
      "Test Episode 54: Reward = -18.00, Steps = 3, Goal Reached: No\n",
      "Test Episode 55: Reward = -156.00, Steps = 39, Goal Reached: No\n",
      "Test Episode 56: Reward = -450.00, Steps = 122, Goal Reached: Yes\n",
      "Test Episode 57: Reward = -583.00, Steps = 122, Goal Reached: No\n",
      "Test Episode 58: Reward = -18.00, Steps = 3, Goal Reached: No\n",
      "Test Episode 59: Reward = -240.00, Steps = 51, Goal Reached: No\n",
      "Test Episode 60: Reward = -235.00, Steps = 79, Goal Reached: Yes\n",
      "Test Episode 61: Reward = -804.00, Steps = 194, Goal Reached: Yes\n",
      "Test Episode 62: Reward = -789.00, Steps = 191, Goal Reached: Yes\n",
      "Test Episode 63: Reward = -450.00, Steps = 122, Goal Reached: Yes\n",
      "Test Episode 64: Reward = -297.00, Steps = 89, Goal Reached: Yes\n",
      "Test Episode 65: Reward = -571.00, Steps = 145, Goal Reached: Yes\n",
      "Test Episode 66: Reward = -655.00, Steps = 163, Goal Reached: Yes\n",
      "Test Episode 67: Reward = -471.00, Steps = 102, Goal Reached: No\n",
      "Test Episode 68: Reward = -722.00, Steps = 180, Goal Reached: Yes\n",
      "Test Episode 69: Reward = -604.00, Steps = 154, Goal Reached: Yes\n",
      "Test Episode 70: Reward = -170.00, Steps = 66, Goal Reached: Yes\n",
      "Test Episode 71: Reward = -154.00, Steps = 64, Goal Reached: Yes\n",
      "Test Episode 72: Reward = -1432.00, Steps = 322, Goal Reached: Yes\n",
      "Test Episode 73: Reward = -79.00, Steps = 20, Goal Reached: No\n",
      "Test Episode 74: Reward = -146.00, Steps = 31, Goal Reached: No\n",
      "Test Episode 75: Reward = -2330.00, Steps = 498, Goal Reached: Yes\n",
      "Test Episode 76: Reward = -904.00, Steps = 185, Goal Reached: No\n",
      "Test Episode 77: Reward = -337.00, Steps = 74, Goal Reached: No\n",
      "Test Episode 78: Reward = -1154.00, Steps = 264, Goal Reached: Yes\n",
      "Test Episode 79: Reward = -394.00, Steps = 112, Goal Reached: Yes\n",
      "Test Episode 80: Reward = -2869.00, Steps = 607, Goal Reached: Yes\n",
      "Test Episode 81: Reward = -18.00, Steps = 3, Goal Reached: No\n",
      "Test Episode 82: Reward = -423.00, Steps = 90, Goal Reached: No\n",
      "Test Episode 83: Reward = -1924.00, Steps = 418, Goal Reached: Yes\n",
      "Test Episode 84: Reward = -175.00, Steps = 67, Goal Reached: Yes\n",
      "Test Episode 85: Reward = -4635.00, Steps = 936, Goal Reached: No\n",
      "Test Episode 86: Reward = -151.00, Steps = 61, Goal Reached: Yes\n",
      "Test Episode 87: Reward = -337.00, Steps = 97, Goal Reached: Yes\n",
      "Test Episode 88: Reward = -243.00, Steps = 54, Goal Reached: No\n",
      "Test Episode 89: Reward = -160.00, Steps = 64, Goal Reached: Yes\n",
      "Test Episode 90: Reward = -1179.00, Steps = 269, Goal Reached: Yes\n",
      "Test Episode 91: Reward = -210.00, Steps = 74, Goal Reached: Yes\n",
      "Test Episode 92: Reward = -724.00, Steps = 178, Goal Reached: Yes\n",
      "Test Episode 93: Reward = -1579.00, Steps = 349, Goal Reached: Yes\n",
      "Test Episode 94: Reward = 53.00, Steps = 19, Goal Reached: Yes\n",
      "Test Episode 95: Reward = -371.00, Steps = 105, Goal Reached: Yes\n",
      "Test Episode 96: Reward = -819.00, Steps = 197, Goal Reached: Yes\n",
      "Test Episode 97: Reward = -18.00, Steps = 3, Goal Reached: No\n",
      "Test Episode 98: Reward = -1615.00, Steps = 355, Goal Reached: Yes\n",
      "Test Episode 99: Reward = -1170.00, Steps = 243, Goal Reached: No\n",
      "Test Episode 100: Reward = -1349.00, Steps = 303, Goal Reached: Yes\n",
      "Test Episode 101: Reward = -300.00, Steps = 92, Goal Reached: Yes\n",
      "Test Episode 102: Reward = -1025.00, Steps = 237, Goal Reached: Yes\n",
      "Test Episode 103: Reward = -231.00, Steps = 77, Goal Reached: Yes\n",
      "Test Episode 104: Reward = -18.00, Steps = 3, Goal Reached: No\n",
      "Test Episode 105: Reward = -18.00, Steps = 3, Goal Reached: No\n",
      "Test Episode 106: Reward = -14.00, Steps = 36, Goal Reached: Yes\n",
      "Test Episode 107: Reward = -9.00, Steps = 35, Goal Reached: Yes\n",
      "Test Episode 108: Reward = -2579.00, Steps = 549, Goal Reached: Yes\n",
      "Test Episode 109: Reward = -18.00, Steps = 3, Goal Reached: No\n",
      "Test Episode 110: Reward = -238.00, Steps = 82, Goal Reached: Yes\n",
      "Test Episode 111: Reward = -185.00, Steps = 69, Goal Reached: Yes\n",
      "Test Episode 112: Reward = -595.00, Steps = 128, Goal Reached: No\n",
      "Test Episode 113: Reward = -824.00, Steps = 198, Goal Reached: Yes\n",
      "Test Episode 114: Reward = -2139.00, Steps = 461, Goal Reached: Yes\n",
      "Test Episode 115: Reward = -432.00, Steps = 93, Goal Reached: No\n",
      "Test Episode 116: Reward = -221.00, Steps = 75, Goal Reached: Yes\n",
      "Test Episode 117: Reward = -2044.00, Steps = 442, Goal Reached: Yes\n",
      "Test Episode 118: Reward = -1684.00, Steps = 370, Goal Reached: Yes\n",
      "Test Episode 119: Reward = -285.00, Steps = 89, Goal Reached: Yes\n",
      "Test Episode 120: Reward = -338.00, Steps = 102, Goal Reached: Yes\n",
      "Test Episode 121: Reward = -237.00, Steps = 77, Goal Reached: Yes\n",
      "Test Episode 122: Reward = -155.00, Steps = 63, Goal Reached: Yes\n",
      "Test Episode 123: Reward = -2794.00, Steps = 592, Goal Reached: Yes\n",
      "Test Episode 124: Reward = -2360.00, Steps = 504, Goal Reached: Yes\n",
      "Test Episode 125: Reward = -1740.00, Steps = 380, Goal Reached: Yes\n",
      "Test Episode 126: Reward = -175.00, Steps = 67, Goal Reached: Yes\n",
      "Test Episode 127: Reward = -945.00, Steps = 221, Goal Reached: Yes\n",
      "Test Episode 128: Reward = -230.00, Steps = 78, Goal Reached: Yes\n",
      "Test Episode 129: Reward = -28.00, Steps = 5, Goal Reached: No\n",
      "Test Episode 130: Reward = -181.00, Steps = 67, Goal Reached: Yes\n",
      "Test Episode 131: Reward = -699.00, Steps = 173, Goal Reached: Yes\n",
      "Test Episode 132: Reward = -151.00, Steps = 61, Goal Reached: Yes\n",
      "Test Episode 133: Reward = -35.00, Steps = 39, Goal Reached: Yes\n",
      "Test Episode 134: Reward = -181.00, Steps = 67, Goal Reached: Yes\n",
      "Test Episode 135: Reward = -171.00, Steps = 65, Goal Reached: Yes\n",
      "Test Episode 136: Reward = -391.00, Steps = 109, Goal Reached: Yes\n",
      "Test Episode 137: Reward = -1243.00, Steps = 283, Goal Reached: Yes\n",
      "Test Episode 138: Reward = -469.00, Steps = 127, Goal Reached: Yes\n",
      "Test Episode 139: Reward = -565.00, Steps = 145, Goal Reached: Yes\n",
      "Test Episode 140: Reward = -276.00, Steps = 63, Goal Reached: No\n",
      "Test Episode 141: Reward = -785.00, Steps = 189, Goal Reached: Yes\n",
      "Test Episode 142: Reward = -1472.00, Steps = 330, Goal Reached: Yes\n",
      "Test Episode 143: Reward = -16.00, Steps = 5, Goal Reached: No\n",
      "Test Episode 144: Reward = -55.00, Steps = 43, Goal Reached: Yes\n",
      "Test Episode 145: Reward = -794.00, Steps = 192, Goal Reached: Yes\n",
      "Test Episode 146: Reward = -159.00, Steps = 36, Goal Reached: No\n",
      "Test Episode 147: Reward = -227.00, Steps = 75, Goal Reached: Yes\n",
      "Test Episode 148: Reward = -373.00, Steps = 109, Goal Reached: Yes\n",
      "Test Episode 149: Reward = -604.00, Steps = 154, Goal Reached: Yes\n",
      "Test Episode 150: Reward = -28.00, Steps = 5, Goal Reached: No\n",
      "Test Episode 151: Reward = -1105.00, Steps = 253, Goal Reached: Yes\n",
      "Test Episode 152: Reward = -23.00, Steps = 4, Goal Reached: No\n",
      "Test Episode 153: Reward = -614.00, Steps = 156, Goal Reached: Yes\n",
      "Test Episode 154: Reward = -18.00, Steps = 3, Goal Reached: No\n",
      "Test Episode 155: Reward = -378.00, Steps = 81, Goal Reached: No\n",
      "Test Episode 156: Reward = -279.00, Steps = 60, Goal Reached: No\n",
      "Test Episode 157: Reward = -277.00, Steps = 85, Goal Reached: Yes\n",
      "Test Episode 158: Reward = -82.00, Steps = 46, Goal Reached: Yes\n",
      "Test Episode 159: Reward = -1508.00, Steps = 336, Goal Reached: Yes\n",
      "Test Episode 160: Reward = -774.00, Steps = 188, Goal Reached: Yes\n",
      "Test Episode 161: Reward = -70.00, Steps = 46, Goal Reached: Yes\n",
      "Test Episode 162: Reward = -1218.00, Steps = 278, Goal Reached: Yes\n",
      "Test Episode 163: Reward = -721.00, Steps = 152, Goal Reached: No\n",
      "Test Episode 164: Reward = -780.00, Steps = 188, Goal Reached: Yes\n",
      "Test Episode 165: Reward = 18.00, Steps = 26, Goal Reached: Yes\n",
      "Test Episode 166: Reward = -18.00, Steps = 3, Goal Reached: No\n",
      "Test Episode 167: Reward = -543.00, Steps = 143, Goal Reached: Yes\n",
      "Test Episode 168: Reward = -636.00, Steps = 158, Goal Reached: Yes\n",
      "Test Episode 169: Reward = -176.00, Steps = 66, Goal Reached: Yes\n",
      "Test Episode 170: Reward = -2408.00, Steps = 516, Goal Reached: Yes\n",
      "Test Episode 171: Reward = -444.00, Steps = 122, Goal Reached: Yes\n",
      "Test Episode 172: Reward = -219.00, Steps = 77, Goal Reached: Yes\n",
      "Test Episode 173: Reward = -127.00, Steps = 55, Goal Reached: Yes\n",
      "Test Episode 174: Reward = -488.00, Steps = 132, Goal Reached: Yes\n",
      "Test Episode 175: Reward = -478.00, Steps = 130, Goal Reached: Yes\n",
      "Test Episode 176: Reward = -584.00, Steps = 150, Goal Reached: Yes\n",
      "Test Episode 177: Reward = -195.00, Steps = 71, Goal Reached: Yes\n",
      "Test Episode 178: Reward = -3145.00, Steps = 661, Goal Reached: Yes\n",
      "Test Episode 179: Reward = -1393.00, Steps = 313, Goal Reached: Yes\n",
      "Test Episode 180: Reward = -210.00, Steps = 74, Goal Reached: Yes\n",
      "Test Episode 181: Reward = -1571.00, Steps = 322, Goal Reached: No\n",
      "Test Episode 182: Reward = -343.00, Steps = 74, Goal Reached: No\n",
      "Test Episode 183: Reward = -619.00, Steps = 157, Goal Reached: Yes\n",
      "Test Episode 184: Reward = -515.00, Steps = 112, Goal Reached: No\n",
      "Test Episode 185: Reward = -18.00, Steps = 3, Goal Reached: No\n",
      "Test Episode 186: Reward = -614.00, Steps = 156, Goal Reached: Yes\n",
      "Test Episode 187: Reward = -517.00, Steps = 110, Goal Reached: No\n",
      "Test Episode 188: Reward = -1470.00, Steps = 326, Goal Reached: Yes\n",
      "Test Episode 189: Reward = -814.00, Steps = 196, Goal Reached: Yes\n",
      "Test Episode 190: Reward = -915.00, Steps = 215, Goal Reached: Yes\n",
      "Test Episode 191: Reward = -421.00, Steps = 92, Goal Reached: No\n",
      "Test Episode 192: Reward = -154.00, Steps = 64, Goal Reached: Yes\n",
      "Test Episode 193: Reward = -1149.00, Steps = 234, Goal Reached: No\n",
      "Test Episode 194: Reward = -197.00, Steps = 69, Goal Reached: Yes\n",
      "Test Episode 195: Reward = -1499.00, Steps = 333, Goal Reached: Yes\n",
      "Test Episode 196: Reward = -157.00, Steps = 61, Goal Reached: Yes\n",
      "Test Episode 197: Reward = -299.00, Steps = 93, Goal Reached: Yes\n",
      "Test Episode 198: Reward = -18.00, Steps = 3, Goal Reached: No\n",
      "Test Episode 199: Reward = -18.00, Steps = 3, Goal Reached: No\n",
      "Test Episode 200: Reward = -17.00, Steps = 4, Goal Reached: No\n",
      "\n",
      "--- Testing Metrics ---\n",
      "Average Reward: -635.47\n",
      "Success Rate: 69.50%\n",
      "Average Steps to Goal (for successful episodes): 176.59\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the testing loop\n",
    "DQN_Backward_planning_metrics = test_model_with_backward(policy_net, backward_model, env, num_test_episodes=200, use_backward=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Success Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_success_rate(model, env_test, num_episodes=100):\n",
    "    success_count = 0\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = one_hot_state(state, state_space)\n",
    "        history = deque([np.zeros(state_space) for _ in range(history_size)], maxlen=history_size)\n",
    "        rewards = deque([0 for _ in range(history_size)], maxlen=history_size)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Prepare tensors for history and rewards\n",
    "            history_tensor = torch.tensor(np.array(history), dtype=torch.float32).unsqueeze(0)\n",
    "            rewards_tensor = torch.tensor(np.array(rewards), dtype=torch.float32).unsqueeze(-1).unsqueeze(0)\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action = torch.argmax(model(state_tensor, history_tensor, rewards_tensor)).item()\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            state = one_hot_state(next_state, state_space)\n",
    "\n",
    "            # Update history and rewards\n",
    "            history.append(state)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if done and reward > 0:  # Reached the goal\n",
    "                success_count += 1\n",
    "\n",
    "    success_rate = (success_count / num_episodes) * 100\n",
    "    return success_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Average Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_reward(model, env_test, num_episodes=100):\n",
    "    total_reward = 0\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = one_hot_state(state, state_space)\n",
    "        history = deque([np.zeros(state_space) for _ in range(history_size)], maxlen=history_size)\n",
    "        rewards = deque([0 for _ in range(history_size)], maxlen=history_size)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Prepare tensors for history and rewards\n",
    "            history_tensor = torch.tensor(np.array(history), dtype=torch.float32).unsqueeze(0)\n",
    "            rewards_tensor = torch.tensor(np.array(rewards), dtype=torch.float32).unsqueeze(-1).unsqueeze(0)\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action = torch.argmax(model(state_tensor, history_tensor, rewards_tensor)).item()\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            state = one_hot_state(next_state, state_space)\n",
    "\n",
    "            # Update history and rewards\n",
    "            history.append(state)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    return average_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Steps taken to goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_steps_to_goal(model, env_test, num_episodes=100):\n",
    "    total_steps = 0\n",
    "    success_count = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = one_hot_state(state, state_space)\n",
    "        history = deque([np.zeros(state_space) for _ in range(history_size)], maxlen=history_size)\n",
    "        rewards = deque([0 for _ in range(history_size)], maxlen=history_size)\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            # Prepare tensors for history and rewards\n",
    "            history_tensor = torch.tensor(np.array(history), dtype=torch.float32).unsqueeze(0)\n",
    "            rewards_tensor = torch.tensor(np.array(rewards), dtype=torch.float32).unsqueeze(-1).unsqueeze(0)\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action = torch.argmax(model(state_tensor, history_tensor, rewards_tensor)).item()\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            state = one_hot_state(next_state, state_space)\n",
    "\n",
    "            # Update history and rewards\n",
    "            history.append(state)\n",
    "            rewards.append(reward)\n",
    "            steps += 1\n",
    "\n",
    "            if done and reward > 0:  # Reached the goal\n",
    "                total_steps += steps\n",
    "                success_count += 1\n",
    "\n",
    "    if success_count == 0:\n",
    "        return None  # No successful episodes\n",
    "    return total_steps / success_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # Clear unused GPU memory before plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m num_eval_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Evaluate the Attention-DQN model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m success_rate \u001b[38;5;241m=\u001b[39m calculate_success_rate(policy_net, \u001b[43menv_test\u001b[49m, num_eval_episodes)\n\u001b[0;32m      4\u001b[0m average_reward \u001b[38;5;241m=\u001b[39m calculate_average_reward(policy_net, env_test, num_eval_episodes)\n\u001b[0;32m      5\u001b[0m steps_to_goal \u001b[38;5;241m=\u001b[39m calculate_steps_to_goal(policy_net, env_test, num_eval_episodes)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env_test' is not defined"
     ]
    }
   ],
   "source": [
    "num_eval_episodes = 100\n",
    "# Evaluate the Attention-DQN model\n",
    "success_rate = calculate_success_rate(policy_net, env_test, num_eval_episodes)\n",
    "average_reward = calculate_average_reward(policy_net, env_test, num_eval_episodes)\n",
    "steps_to_goal = calculate_steps_to_goal(policy_net, env_test, num_eval_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of metrics\n",
    "metrics = ['Success Rate (%)', 'Average Reward', 'Average Steps to Goal']\n",
    "values = [success_rate, average_reward, steps_to_goal if steps_to_goal is not None else 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # Clear unused GPU memory before plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(metrics, values, color=['blue', 'green', 'orange'])\n",
    "plt.title('Performance Metrics of the Attention-Enhanced Model')\n",
    "plt.ylabel('Values')\n",
    "plt.ylim(0, max(values) + 10)  # Adjust Y-axis limits based on metrics\n",
    "for i, value in enumerate(values):\n",
    "    plt.text(i, value + 1, f'{value:.2f}', ha='center', fontsize=12)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Success Rate: {success_rate:.2f}%\")\n",
    "print(f\"Average Reward: {average_reward:.2f}\")\n",
    "print(f\"Average Steps to Goal: {steps_to_goal if steps_to_goal is not None else 'No successful episodes'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming both models are trained: policy_net (Original DQN) and attention_policy_net (Attention DQN)\n",
    "\n",
    "# Evaluate both models\n",
    "# num_episodes = 100  # Number of test episodes\n",
    "\n",
    "# # Original DQN\n",
    "# success_rate_dqn = calculate_success_rate(policy_net, env_test, num_episodes)\n",
    "# average_reward_dqn = calculate_average_reward(policy_net, env_test, num_episodes)\n",
    "# steps_to_goal_dqn = calculate_steps_to_goal(policy_net, env_test, num_episodes)\n",
    "# num_episodes = 100\n",
    "# # # Attention DQN\n",
    "# success_rate_attention_dqn = calculate_success_rate(policy_net, env_test, num_episodes)\n",
    "# average_reward_attention_dqn = calculate_average_reward(policy_net, env_test, num_episodes)\n",
    "# steps_to_goal_attention_dqn = calculate_steps_to_goal(policy_net, env_test, num_episodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Prepare for plotting\n",
    "# metrics = ['Success Rate (%)', 'Average Reward', 'Steps to Goal']\n",
    "# # dqn_values = [success_rate_dqn, average_reward_dqn, steps_to_goal_dqn]\n",
    "# attention_dqn_values = [success_rate_attention_dqn, average_reward_attention_dqn, steps_to_goal_attention_dqn]\n",
    "\n",
    "# # # Plotting each metric\n",
    "# # fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# # # Success Rate Comparison\n",
    "# axs[0].bar(['Original DQN', 'Attention DQN'], [success_rate_dqn, success_rate_attention_dqn], color=['blue', 'green'])\n",
    "# axs[0].set_title('Success Rate Comparison')\n",
    "# axs[0].set_ylabel('Success Rate (%)')\n",
    "\n",
    "# # Average Reward Comparison\n",
    "# axs[1].bar(['Original DQN', 'Attention DQN'], [average_reward_dqn, average_reward_attention_dqn], color=['blue', 'green'])\n",
    "# axs[1].set_title('Average Reward Comparison')\n",
    "# axs[1].set_ylabel('Average Reward')\n",
    "\n",
    "# # Steps to Goal Comparison\n",
    "# axs[2].bar(['Original DQN', 'Attention DQN'], [steps_to_goal_dqn, steps_to_goal_attention_dqn], color=['blue', 'green'])\n",
    "# axs[2].set_title('Steps to Goal Comparison')\n",
    "# axs[2].set_ylabel('Steps to Goal')\n",
    "\n",
    "# Show the plot\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
